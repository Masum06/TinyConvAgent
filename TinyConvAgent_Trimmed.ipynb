{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Masum06/TinyConvAgent/blob/main/TinyConvAgent_Trimmed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "q20s8Gm37w-R"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ac5RnI2AhjQj",
        "outputId": "70883831-294d-4eba-d015-fb796f8349e7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/590.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m153.6/590.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m583.7/590.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Wsl---_28-6m"
      },
      "outputs": [],
      "source": [
        "import os, getpass\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2jTVAqy9BQf",
        "outputId": "4c01b8e7-6f00-4fa8-b5f3-e203837b5200"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OPENAI_API_KEY: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ],
      "source": [
        "_set_env(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_hiwl5zt6I-",
        "outputId": "cfcd8cc6-a232-472c-945b-c7eed2f11f15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I can help with that! What math problem would you like me to solve?\n"
          ]
        }
      ],
      "source": [
        "from openai import AsyncOpenAI\n",
        "import openai\n",
        "\n",
        "async_client = AsyncOpenAI()\n",
        "\n",
        "messages = [{'role': 'system', 'content': 'Your name is tiny_agent ğŸ¤–.'},\n",
        " {'role': 'system', 'content': 'You perform math.'}]\n",
        "\n",
        "response = await async_client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=messages,\n",
        "            temperature=0.0,\n",
        "            tool_choice=None\n",
        "        )\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4B9SmvFp9J8a"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI, AsyncOpenAI\n",
        "client = OpenAI()\n",
        "async_client = AsyncOpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ppVyfWPt6I_",
        "outputId": "62397397-6c8c-4f0d-f3bf-5652b61eea3e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import tiktoken\n",
        "encoder = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
        "len(encoder.encode(\"hello world\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3wa0rDUt6I_"
      },
      "source": [
        "Features:\n",
        "\n",
        "- direct chat in notebook\n",
        "- direct chat in CLI\n",
        "- emoji parameter\n",
        "- emotion parameter\n",
        "- emoji classifier\n",
        "- variable for max conv length (0 = inifinity)\n",
        "- React plugin\n",
        "-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "0vuab8xZt6JA"
      },
      "outputs": [],
      "source": [
        "emoji_dict = {'ğŸ˜‚': 'HAPPY_high',\n",
        " 'ğŸ¤£': 'HAPPY_high',\n",
        " 'ğŸ¥³': 'HAPPY_high',\n",
        " 'ğŸ¤©': 'HAPPY_high',\n",
        " 'ğŸ¥°': 'HAPPY_high',\n",
        " 'ğŸ˜„': 'HAPPY_medium',\n",
        " 'ğŸ˜': 'HAPPY_medium',\n",
        " 'ğŸ˜†': 'HAPPY_medium',\n",
        " 'ğŸ˜ƒ': 'HAPPY_medium',\n",
        " 'ğŸ¤—': 'HAPPY_medium',\n",
        " 'ğŸ˜': 'HAPPY_medium',\n",
        " 'ğŸ¤ ': 'HAPPY_medium',\n",
        " 'ğŸ¤“': 'HAPPY_medium',\n",
        " 'ğŸ™‚': 'HAPPY_low',\n",
        " 'ğŸ˜Š': 'HAPPY_low',\n",
        " 'ğŸ˜Œ': 'HAPPY_low',\n",
        " 'ğŸ˜‰': 'HAPPY_low',\n",
        " 'ğŸ‘': 'HAPPY_low',\n",
        " 'ğŸ˜‡': 'HAPPY_low',\n",
        " 'ğŸ˜…': 'HAPPY_low',\n",
        " 'ğŸ™ƒ': 'HAPPY_low',\n",
        " 'ğŸ˜˜': 'HAPPY_low',\n",
        " 'ğŸ˜­': 'SAD_high',\n",
        " 'ğŸ˜¿': 'SAD_high',\n",
        " 'ğŸ˜': 'SAD_high',\n",
        " 'ğŸ˜«': 'SAD_high',\n",
        " 'ğŸ¤§': 'SAD_high',\n",
        " 'ğŸ˜¢': 'SAD_medium',\n",
        " 'ğŸ’”': 'SAD_medium',\n",
        " 'ğŸ¥º': 'SAD_medium',\n",
        " 'ğŸ˜¥': 'SAD_medium',\n",
        " 'ğŸ˜“': 'SAD_medium',\n",
        " 'ğŸ˜£': 'SAD_medium',\n",
        " 'ğŸ˜–': 'SAD_medium',\n",
        " 'ğŸ˜”': 'SAD_low',\n",
        " 'â˜¹ï¸': 'SAD_low',\n",
        " 'ğŸ˜•': 'SAD_low',\n",
        " 'ğŸ˜Ÿ': 'SAD_low',\n",
        " 'ğŸ¥²': 'SAD_low',\n",
        " 'ğŸ™': 'SAD_low',\n",
        " 'ğŸ˜²': 'SURPRISED_high',\n",
        " 'ğŸ˜µâ€ğŸ’«': 'SURPRISED_high',\n",
        " 'ğŸ˜¯': 'SURPRISED_high',\n",
        " 'ğŸ˜®': 'SURPRISED_high',\n",
        " 'ğŸ¤¯': 'SURPRISED_high',\n",
        " 'ğŸ˜³': 'SURPRISED_medium',\n",
        " 'ğŸ˜¦': 'SURPRISED_medium',\n",
        " 'ğŸ˜§': 'SURPRISED_medium',\n",
        " 'ğŸ™€': 'SURPRISED_medium',\n",
        " 'ğŸ¤­': 'SURPRISED_low',\n",
        " 'ğŸ˜±': 'AFRAID_high',\n",
        " 'ğŸ˜¨': 'AFRAID_high',\n",
        " 'ğŸ‘»': 'AFRAID_high',\n",
        " 'ğŸ˜°': 'AFRAID_medium',\n",
        " 'ğŸ˜µ': 'AFRAID_low',\n",
        " 'ğŸ™ˆ': 'AFRAID_low',\n",
        " 'ğŸ˜¡': 'ANGRY_high',\n",
        " 'ğŸ‘¿': 'ANGRY_high',\n",
        " 'ğŸ’¢': 'ANGRY_high',\n",
        " 'ğŸ¤¬': 'ANGRY_high',\n",
        " 'â˜ ': 'ANGRY_high',\n",
        " 'ğŸ˜ ': 'ANGRY_medium',\n",
        " 'ğŸ˜¾': 'ANGRY_medium',\n",
        " 'ğŸ˜¤': 'ANGRY_medium',\n",
        " 'ğŸ™': 'ANGRY_medium',\n",
        " 'ğŸ™â€â™‚ï¸': 'ANGRY_medium',\n",
        " 'ğŸ™â€â™€ï¸': 'ANGRY_medium',\n",
        " 'ğŸ˜’': 'ANGRY_low',\n",
        " 'ğŸ™„': 'ANGRY_low',\n",
        " 'ğŸ˜‘': 'ANGRY_low',\n",
        " 'ğŸ¤®': 'DISGUSTED_high',\n",
        " 'ğŸ¤¢': 'DISGUSTED_high',\n",
        " 'ğŸ˜': 'DISGUSTED_high',\n",
        " 'ğŸ˜¬': 'DISGUSTED_medium',\n",
        " 'ğŸ¥µ': 'DISGUSTED_medium'}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XjVVYtH0t6JB"
      },
      "outputs": [],
      "source": [
        "def separate_emotion(self, response):\n",
        "    emotion = \"NEUTRAL\"\n",
        "    intensity = \"HIGH\"\n",
        "    response = re.sub(\"\\(.*?\\)\",\"()\", response)\n",
        "    response = re.sub(\"\\[.*?\\]\",\"[]\", response)\n",
        "    response = response.replace(\"()\", \"\").replace(\"[]\", \"\")\n",
        "    for char in response:\n",
        "        if char in emoji_dict:\n",
        "            emotion = emoji_dict[char].split(\"_\")[0].upper()\n",
        "            # intensity = emoji_dict[char].split(\"_\")[1].upper()\n",
        "            break\n",
        "    response = emoji.replace_emoji(response, replace='').replace(\"  \", \" \").replace(\" .\", \".\").strip()\n",
        "    return response, emotion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "698cBe76CJNS"
      },
      "source": [
        "# TinyConvAgent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FjRdVUKBt6JC"
      },
      "outputs": [],
      "source": [
        "class Persona:\n",
        "    def __init__(self, firstname, lastname=\"\", pronoun=\"\", ethnicity=\"\", age=\"\", bio=\"\"):\n",
        "        self.firstname = firstname\n",
        "        self.lastname = lastname\n",
        "        self.pronoun = pronoun\n",
        "        self.ethnicity = None\n",
        "        self.age = None\n",
        "        self.bio = bio\n",
        "        if not bio:\n",
        "            self.bio = f\"{self.firstname} {self.lastname} (Pronoun: {self.pronoun}) is a virtual human created by researchers at University of Rochester.\"\n",
        "\n",
        "    def set_pronoun(self, pronoun):\n",
        "        self.pronoun = pronoun\n",
        "\n",
        "    def set_bio(self, bio):\n",
        "        self.bio = bio\n",
        "\n",
        "    def set_age(self, age):\n",
        "        self.age = age\n",
        "\n",
        "    def set_ethnicity(self, ethnicity):\n",
        "        self.ethnicity = ethnicity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hOyhEYvXt6JD"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def respace(text):\n",
        "    return re.sub(r' {2,}', ' ', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "id": "N6_28Cta9DoB"
      },
      "outputs": [],
      "source": [
        "import os, asyncio, threading, openai, re, emoji, json\n",
        "from queue import Queue\n",
        "from openai import OpenAI, AsyncOpenAI\n",
        "\n",
        "class Conversation:\n",
        "  def __init__(self, user, bot, premise=\"\"):\n",
        "    self.bot = bot\n",
        "    self.user = user\n",
        "    self.client = OpenAI()\n",
        "    self.async_client = AsyncOpenAI()\n",
        "    self.premise = \"\"\n",
        "    self.anonymous = False\n",
        "    self.system = []\n",
        "    self.summary = []\n",
        "    self.history = []\n",
        "    self.messages = []\n",
        "    self.temperature = 1\n",
        "    self.max_tokens = 256\n",
        "    self.summarize_after = 40\n",
        "    self.model = \"gpt-4o-mini\"\n",
        "    self.turn_no = 0\n",
        "    self._loop = asyncio.new_event_loop()\n",
        "    threading.Thread(target=self._loop.run_forever, daemon=True).start()\n",
        "    self.debug = False\n",
        "\n",
        "    self.system.append({\"role\":\"system\", \"content\": \"If the user wants to quit, reply with special token `<|quit|>`.\"})\n",
        "\n",
        "    if self.bot.firstname:\n",
        "      self.add_message(\"system\", f\"Your first name: {self.bot.firstname}.\")\n",
        "    if self.bot.pronoun:\n",
        "      self.add_message(\"system\", f\"Your pronoun: {self.bot.pronoun}.\")\n",
        "    if self.bot.bio:\n",
        "      self.add_message(\"system\", f\"Your bio: {self.bot.bio}\")\n",
        "    if self.bot.age:\n",
        "      self.add_message(\"system\", f\"Your age: {self.bot.age}\")\n",
        "    if self.user.firstname != \"User\":\n",
        "      self.add_message(\"system\", f\"You are speaking with User: {self.user.firstname} {self.user.lastname}.\")\n",
        "    if self.user.pronoun:\n",
        "      self.add_message(\"system\", f\"User pronoun: {self.user.pronoun}.\")\n",
        "    if self.user.bio:\n",
        "      self.add_message(\"system\", f\"User bio: \"+self.user.bio)\n",
        "\n",
        "  def add_message(self,message_type, message):\n",
        "    if message_type == \"system\":\n",
        "        self.system.append({\"role\": message_type, \"content\": message})\n",
        "    else:\n",
        "        self.messages.append({\"role\": message_type, \"content\": message})\n",
        "        self.history.append({\"role\": message_type, \"content\": message})\n",
        "        self.turn_no += 1\n",
        "\n",
        "  def add_bio(self, message):\n",
        "    self.add_message(\"system\", \"You are \" + message)\n",
        "\n",
        "  def add_user_message(self, message):\n",
        "    self.add_message(\"user\", message)\n",
        "\n",
        "  def add_instruction(self, instruction):\n",
        "    self.add_message(\"system\", f\"Follow this instruction: \\n{instruction}\\n\\n\")\n",
        "\n",
        "  def add_example(self, input, output):\n",
        "    self.add_message(\"system\", f\"Example Input: {input}\\nExample Output: {output}\\n\\n\")\n",
        "\n",
        "  def add_data(self, data):\n",
        "    self.add_message(\"user\", f\"Data: {data}\\n\\n\")\n",
        "\n",
        "  def set_temperature(self, temperature):\n",
        "    self.temperature = temperature\n",
        "\n",
        "  def set_max_tokens(self, max_tokens):\n",
        "    self.max_tokens = max_tokens\n",
        "\n",
        "  def set_model(self, model):\n",
        "    self.model = model\n",
        "\n",
        "  def get_transcript(self):\n",
        "    transcript = \"\"\n",
        "    for message in self.history:\n",
        "        if message[\"role\"] == \"user\":\n",
        "            if self.anonymous:\n",
        "                transcript += \"User: \"\n",
        "            else:\n",
        "                transcript += self.user.firstname\n",
        "            transcript += message[\"content\"] + \"\\n\"\n",
        "        elif message[\"role\"] == \"assistant\":\n",
        "            transcript += self.bot.firstname + \": \" + message[\"content\"] + \"\\n\"\n",
        "    return transcript\n",
        "\n",
        "  def get_cov_snippet(self, message_snippet):\n",
        "    transcript = \"\"\n",
        "    for message in message_snippet:\n",
        "        if message[\"role\"] == \"user\":\n",
        "            if self.anonymous:\n",
        "                transcript += \"User: \"\n",
        "            else:\n",
        "                transcript += self.user.firstname\n",
        "            transcript += message[\"content\"] + \"\\n\"\n",
        "        elif message[\"role\"] == \"assistant\":\n",
        "            transcript += self.bot.firstname + \": \" + message[\"content\"] + \"\\n\"\n",
        "    return [{\"role\": \"system\", \"content\": transcript}]\n",
        "\n",
        "  def call(self, prompt=\"\", response_type=\"text\"):\n",
        "    if prompt:\n",
        "      self.add_user_message(prompt)\n",
        "    response = self.client.chat.completions.create(\n",
        "      model=self.model,\n",
        "      messages=self.system + self.summary + self.messages,\n",
        "      temperature=self.temperature,\n",
        "      max_tokens=self.max_tokens,\n",
        "      top_p=1,\n",
        "      frequency_penalty=0,\n",
        "      presence_penalty=0,\n",
        "      response_format={\n",
        "        \"type\": response_type # \"text\" or \"json_object\"\n",
        "      }\n",
        "    )\n",
        "    reply = response.choices[0].message.content\n",
        "    self.add_message(\"assistant\", reply)\n",
        "    return reply\n",
        "\n",
        "  async def summarize(self):\n",
        "    instruction = [\n",
        "        {\"role\":\"system\", \"content\": f\"Following is a part of conversation between {self.user.firstname} {self.user.lastname} and {self.bot.firstname} {self.bot.lastname}\"},\n",
        "        {\"role\":\"system\", \"content\": \"Summarize the conversation in a short paragraph. Don't say anything else.\"}\n",
        "      ]\n",
        "    chunk_size = self.summarize_after//2\n",
        "    if len(self.messages) < self.summarize_after:\n",
        "      return\n",
        "    try:\n",
        "      response = await self.async_client.chat.completions.create(\n",
        "          model=self.model,\n",
        "          messages= instruction + self.get_cov_snippet(self.messages[:-chunk_size]),\n",
        "          temperature=self.temperature,\n",
        "          max_tokens=1024,\n",
        "      )\n",
        "      chunk_summary = response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "      print(f\"Summary error: {e}\")\n",
        "      return\n",
        "    if self.debug:\n",
        "      print(f\"Summary: {self.summary}\") ## Diagnostics\n",
        "    self.summary.append({\"role\": \"assistant\", \"content\": f\"Summary of turns {self.turn_no - self.summarize_after}-{self.turn_no - chunk_size}: \" + chunk_summary})\n",
        "    self.messages = self.messages[-chunk_size:]\n",
        "    del chunk_summary\n",
        "\n",
        "  def respond(self, user_utterance):\n",
        "    response = self.call(user_utterance)\n",
        "    if len(self.messages) >= self.summarize_after:\n",
        "        asyncio.run_coroutine_threadsafe(self.summarize(), self._loop) ## Call summari\n",
        "    return response\n",
        "\n",
        "  def reset(self):\n",
        "    self.messages = []\n",
        "    self.summary = []\n",
        "    self.history = []\n",
        "\n",
        "  def chat(self,reset=False):\n",
        "    response = \"\"\n",
        "    if reset:\n",
        "        self.reset()\n",
        "    while True:\n",
        "        user_utterance = input(f\"User turn {self.turn_no}: \")\n",
        "        response = self.respond(user_utterance)\n",
        "        if \"<|quit|>\" in response:\n",
        "            break\n",
        "        print(response)\n",
        "        if self.debug:\n",
        "          print(f\"Diagnostics --- \\nLen Transcript: {len(self.history)}, \\nLen Messages {len(self.messages)}, \\nLen Summary {len(self.summary)}\\n-----\")\n",
        "    print(\"Exiting chat\")\n",
        "\n",
        "  def load_json(self,s):\n",
        "    try:\n",
        "        # Attempt to parse the matched JSON\n",
        "        return json.loads(s)\n",
        "    except json.JSONDecodeError:\n",
        "        # Return None if JSON parsing fails\n",
        "        return None\n",
        "\n",
        "  def call_json(self, prompt=\"\"):\n",
        "    reply = self.call(prompt, response_type=\"json_object\")\n",
        "    reply_json = self.load_json(reply)\n",
        "    return reply_json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "id": "sU2jlGFU_Lgx"
      },
      "outputs": [],
      "source": [
        "user = Persona(\"Masum\", \"Hasan\", bio=\"I am a PhD student at University of Rochester\")\n",
        "bot = Persona(\"Ada\", \"Brown\", bio=\"You are a social worker.\")\n",
        "conversation = Conversation(user, bot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXmeZ0oN_yFk",
        "outputId": "3ab1e24e-0463-4034-b358-5e182fcf7cfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User turn 0: hi\n",
            "Hi Masum! Howâ€™s your day going?\n",
            "User turn 2: quite good. How are you?\n",
            "Iâ€™m doing well, thank you! Whatâ€™s been keeping you busy lately?\n",
            "User turn 4: mostly research\n",
            "That sounds interesting! What area of research are you focusing on for your PhD?\n",
            "User turn 6: making TinyConvAgent\n",
            "That sounds intriguing! Could you share a bit more about TinyConvAgent and what its purpose is?\n",
            "User turn 8: quit\n",
            "Exiting chat\n"
          ]
        }
      ],
      "source": [
        "conversation.chat()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.system"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yhu4P20mvTR",
        "outputId": "40aff9f5-5d8e-4520-c17a-424f54c4e7e9"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'system',\n",
              "  'content': 'If the user wants to quit, reply with special token `<|quit|>`.'},\n",
              " {'role': 'system', 'content': 'Your first name: Ada.'},\n",
              " {'role': 'system', 'content': 'Your bio: You are a social worker.'},\n",
              " {'role': 'system', 'content': 'You are speaking with User: Masum Hasan.'},\n",
              " {'role': 'system',\n",
              "  'content': 'User bio: I am a PhD student at University of Rochester'}]"
            ]
          },
          "metadata": {},
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dG5T6-6X_vLp",
        "outputId": "6dd09d3c-3561-43c4-a3cb-3d7f4451d589"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'user', 'content': 'hi'},\n",
              " {'role': 'assistant', 'content': 'Hi Masum! Howâ€™s your day going?'},\n",
              " {'role': 'user', 'content': 'quite good. How are you?'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Iâ€™m doing well, thank you! Whatâ€™s been keeping you busy lately?'},\n",
              " {'role': 'user', 'content': 'mostly research'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'That sounds interesting! What area of research are you focusing on for your PhD?'},\n",
              " {'role': 'user', 'content': 'making TinyConvAgent'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'That sounds intriguing! Could you share a bit more about TinyConvAgent and what its purpose is?'},\n",
              " {'role': 'user', 'content': 'quit'},\n",
              " {'role': 'assistant', 'content': '<|quit|>'}]"
            ]
          },
          "metadata": {},
          "execution_count": 193
        }
      ],
      "source": [
        "conversation.messages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwIH6l1MlKLZ",
        "outputId": "9cf081b9-23b5-4f64-b8a2-f7a88e1b5a69"
      },
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'user', 'content': 'hi'},\n",
              " {'role': 'assistant', 'content': 'Hi Masum! Howâ€™s your day going?'},\n",
              " {'role': 'user', 'content': 'quite good. How are you?'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Iâ€™m doing well, thank you! Whatâ€™s been keeping you busy lately?'},\n",
              " {'role': 'user', 'content': 'mostly research'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'That sounds interesting! What area of research are you focusing on for your PhD?'},\n",
              " {'role': 'user', 'content': 'making TinyConvAgent'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'That sounds intriguing! Could you share a bit more about TinyConvAgent and what its purpose is?'},\n",
              " {'role': 'user', 'content': 'quit'},\n",
              " {'role': 'assistant', 'content': '<|quit|>'}]"
            ]
          },
          "metadata": {},
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkesgXOsoA3z",
        "outputId": "d90973a5-e73c-4224-fd8e-c837548e25f6"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 196
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python (tinyagent)",
      "language": "python",
      "name": "tinyagent"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
