{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Masum06/TinyConvAgent/blob/main/TinyConvAgent_Gemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chat thread: https://chatgpt.com/g/g-p-683801b5c3e0819192b60f23b08c95eb-sentien/c/68b14971-9ad8-8323-b3fd-1ad91ea82956\n",
        "\n",
        "# Code Refactoring Analysis\n",
        "\n",
        "This notebook has been reorganized to improve modularity, readability, and maintainability. Here are the key changes:\n",
        "\n",
        "1.  **Configuration Management (`ChatConfig`):** All settings (model names, prompts, thresholds) have been moved into a dedicated `ChatConfig` class. This separates configuration from the core application logic, making it easier to tune parameters without changing the code.\n",
        "\n",
        "2.  **Decoupled Classes:** The original, monolithic `Conversation` class has been broken down into smaller, specialized classes:\n",
        "    * **`PromptManager`**: Handles all logic related to building the system prompts and formatting messages for the API.\n",
        "    * **`BackgroundProcessor`**: Manages the complex background tasks of summarization and retrospection using `threading` and `asyncio`, completely isolating this from the main conversation flow.\n",
        "    * **`Conversation` (Refactored)**: Now acts as a high-level coordinator, managing state and delegating tasks to the other components. Its code is significantly cleaner and easier to understand.\n",
        "\n",
        "3.  **Centralized Utilities:** Helper functions and constants (like emoji and flag dictionaries) are now grouped together in dedicated sections for better organization.\n",
        "\n",
        "4.  **Improved Readability:** Docstrings and comments have been added to clarify the purpose of each component, especially the concurrency model, making the code easier to follow."
      ],
      "metadata": {
        "id": "intro_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Installation and Setup"
      ],
      "metadata": {
        "id": "73UVT1vGB5Ko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Setup Notebook CSS\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "q20s8Gm37w-R",
        "cellView": "form"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install emoji tiktoken openai"
      ],
      "metadata": {
        "id": "Ac5RnI2AhjQj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a82b74b-006d-410f-ed2e-7353490ad2dd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/590.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/590.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Wsl---_28-6m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6ac71a29-bd9b-4db7-db53-8a15227d3050"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OPENAI_API_KEY: ··········\n"
          ]
        }
      ],
      "source": [
        "import os, getpass\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "_set_env(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Constants and Utility Functions"
      ],
      "metadata": {
        "id": "utils_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import emoji\n",
        "import tiktoken\n",
        "import json\n",
        "import time\n",
        "import asyncio\n",
        "import threading\n",
        "from typing import List, Tuple, Callable, Coroutine, Any\n",
        "from openai import OpenAI, AsyncOpenAI\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "from zoneinfo import ZoneInfo\n",
        "\n",
        "# --- Dictionaries for Emoji and Flag Processing ---\n",
        "\n",
        "REVERSE_EMOJI_DICT = {\n",
        "    'HAPPY_high': ['😂', '🤣', '🥳', '🤩', '🥰'],\n",
        "    'HAPPY_medium': ['😄', '😁', '😆', '😃', '🤗', '😍', '🤠', '🤓'],\n",
        "    'HAPPY_low': ['🙂', '😊', '😌', '😉', '👍', '😇', '😅', '🙃', '😘'],\n",
        "    'SAD_high': ['😭', '😿', '😞', '😫', '🤧'],\n",
        "    'SAD_medium': ['😢', '💔', '🥺', '😥', '😓', '😣', '😖'],\n",
        "    'SAD_low': ['😔', '☹️', '😕', '😟', '🥲', '🙁'],\n",
        "    'SURPRISED_high': ['😲', '😵‍💫', '😯', '😮', '🤯'],\n",
        "    'SURPRISED_medium': ['😳', '😦', '😧', '🙀'],\n",
        "    'SURPRISED_low': ['🤭'],\n",
        "    'AFRAID_high': ['😱', '😨', '👻'],\n",
        "    'AFRAID_medium': ['😰'],\n",
        "    'AFRAID_low': ['😵', '🙈'],\n",
        "    'ANGRY_high': ['😡', '👿', '💢', '🤬', '☠'],\n",
        "    'ANGRY_medium': ['😠', '😾', '😤', '🙎', '🙎‍♂️', '🙎‍♀️'],\n",
        "    'ANGRY_low': ['😒', '🙄', '😑'],\n",
        "    'DISGUSTED_high': ['🤮', '🤢', '😝'],\n",
        "    'DISGUSTED_medium': ['😬', '🥵']\n",
        "}\n",
        "\n",
        "EMOJI_DICT = {emo: emotion for emotion, emojis in REVERSE_EMOJI_DICT.items() for emo in emojis}\n",
        "\n",
        "FLAGS_DICT = {\n",
        "    \"<|quit|>\": \"quit\",\n",
        "    \"<|silence|>\": \"silence\",\n",
        "    \"<|offensive|>\": \"offensive\",\n",
        "    \"<|profanity|>\": \"profanity\",\n",
        "    \"<|offtopic|>\": \"offtopic\",\n",
        "    \"<|sexual|>\": \"sexual\",\n",
        "    \"<|selfharm|>\": \"selfharm\",\n",
        "    \"<|violence|>\": \"violence\",\n",
        "    \"<|suicide|>\": \"suicide\",\n",
        "    \"<|threat|>\": \"threat\"\n",
        "}\n",
        "\n",
        "# --- Text Processing Utility ---\n",
        "\n",
        "def parse_bot_response(text: str) -> Tuple[str, str, List[str]]:\n",
        "    \"\"\"Extracts flags and emotion from a bot's response text.\"\"\"\n",
        "    flag_matches = re.findall(r\"<\\|.*?\\|>\", text)\n",
        "    flags = [FLAGS_DICT[m] for m in flag_matches if m in FLAGS_DICT]\n",
        "\n",
        "    emotion = \"NEUTRAL\"\n",
        "    for char in text:\n",
        "      if char in EMOJI_DICT:\n",
        "        emotion = EMOJI_DICT[char].split(\"_\")[0].upper()\n",
        "        break\n",
        "\n",
        "    cleaned_text = re.sub(r\"<\\|.*?\\|>\", \"\", text)\n",
        "    cleaned_text = emoji.replace_emoji(cleaned_text, replace='')\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
        "\n",
        "    return cleaned_text, emotion, flags"
      ],
      "metadata": {
        "id": "utils_code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "1b7fc6f0-9bc5-443f-da09-6a6250a01593"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Core Components"
      ],
      "metadata": {
        "id": "core_components_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ChatConfig:\n",
        "    \"\"\"Holds all configuration for the conversation agent.\"\"\"\n",
        "    model: str = \"gpt-4.1-mini\"\n",
        "    temperature: float = 1.0\n",
        "    max_tokens: int = 256\n",
        "    summarize_after: int = 40\n",
        "    compress_summary_after: int = 10\n",
        "    debug: bool = False\n",
        "\n",
        "    system2_thinking: bool = True\n",
        "    system2_model: str = \"gpt-4.1-mini\"\n",
        "    system2_rules: str = (\"If the conversation goes off-topic, bring it back. \"\n",
        "                          \"Be socially intelligent, understand user's emotions and speak appropriately.\")\n",
        "\n",
        "class Persona:\n",
        "    def __init__(self, firstname, lastname=\"\", pronoun=\"\", ethnicity=\"\", age=\"\", bio=\"\"):\n",
        "        self.firstname = firstname\n",
        "        self.lastname = lastname\n",
        "        self.pronoun = pronoun\n",
        "        self.ethnicity = None\n",
        "        self.age = None\n",
        "        self.bio = bio\n",
        "        if not bio:\n",
        "            self.bio = f\"{self.firstname} {self.lastname} (Pronoun: {self.pronoun}) is a virtual human created by researchers at University of Rochester.\"\n",
        "\n",
        "    def set_pronoun(self, pronoun):\n",
        "        self.pronoun = pronoun\n",
        "\n",
        "    def set_bio(self, bio):\n",
        "        self.bio = bio\n",
        "\n",
        "    def set_age(self, age):\n",
        "        self.age = age\n",
        "\n",
        "    def set_ethnicity(self, ethnicity):\n",
        "        self.ethnicity = ethnicity"
      ],
      "metadata": {
        "id": "config_class",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "62c28a1c-6235-4f75-8117-1e732f465d5b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "from zoneinfo import ZoneInfo\n",
        "\n",
        "class PromptManager:\n",
        "    \"\"\"Handles the creation and formatting of system prompts.\"\"\"\n",
        "    def __init__(self, bot: Persona, user: Persona, config: ChatConfig):\n",
        "        self.bot = bot\n",
        "        self.user = user\n",
        "        self.config = config\n",
        "\n",
        "    def build_initial_system_prompt(self) -> List[dict]:\n",
        "        \"\"\"Constructs the initial, static system prompt for the conversation.\"\"\"\n",
        "        time_zone = datetime.now(ZoneInfo(\"America/New_York\"))\n",
        "        start_time_str = time_zone.strftime(\"%A, %B %d, %Y %I:%M %p EST\")\n",
        "\n",
        "        system_prompt = [\n",
        "            {\"role\": \"system\", \"content\": \"Don't say that you are an AI Language Model.\"},\n",
        "            {\"role\": \"system\", \"content\": \"Don't let the other speaker talk off topic.\"},\n",
        "            {\"role\": \"system\", \"content\": f\"You are located in the EST time zone. Conversation start time: {start_time_str}\"},\n",
        "            {\"role\": \"system\", \"content\": \"This conversation is happening over a video call. When everyone says goodbye and the conversation ends naturally, say <|quit|> to end it.\"},\n",
        "            {\"role\": \"system\", \"content\": f\"To express {self.bot.firstname}'s emotions, use at most one relevant emoji at the end of your response.\"},\n",
        "            {\"role\": \"system\", \"content\": f\"If the user's statement contains sensitive topics, print one of these flags: {', '.join(list(FLAGS_DICT.keys())[2:])}\"},\n",
        "            {\"role\": \"system\", \"content\": \"To stay silent or skip a turn, say <|silence|>.\"},\n",
        "            {\"role\": \"system\", \"content\": \"Do not say <|quit|> until the user has said goodbye.\"},\n",
        "        ]\n",
        "\n",
        "        # Add persona details\n",
        "        if self.bot.firstname: system_prompt.append({\"role\": \"system\", \"content\": f\"Your first name: {self.bot.firstname}.\"})\n",
        "        if self.bot.pronoun: system_prompt.append({\"role\": \"system\", \"content\": f\"Your pronoun: {self.bot.pronoun}.\"})\n",
        "        if self.bot.bio: system_prompt.append({\"role\": \"system\", \"content\": f\"Your bio: {self.bot.bio}\"})\n",
        "        if self.bot.age: system_prompt.append({\"role\": \"system\", \"content\": f\"Your age: {self.bot.age}\"})\n",
        "        if self.user.firstname != \"User\": system_prompt.append({\"role\": \"system\", \"content\": f\"You are speaking with: {self.user.firstname} {self.user.lastname}.\"})\n",
        "        if self.user.pronoun: system_prompt.append({\"role\": \"system\", \"content\": f\"User's pronoun: {self.user.pronoun}.\"})\n",
        "        if self.user.bio: system_prompt.append({\"role\": \"system\", \"content\": f\"User's bio: {self.user.bio}\"})\n",
        "\n",
        "        # Add System 2 (Retrospection) rules if enabled\n",
        "        if self.config.system2_thinking:\n",
        "            system_prompt.append({\"role\": \"system\", \"content\": \"Instruction: Use the retrospect thinking to improve your next message.\"})\n",
        "            system_prompt.append({\"role\": \"system\", \"content\": f\"[system2] Guidelines:\\n{self.config.system2_rules}\"})\n",
        "\n",
        "        return system_prompt"
      ],
      "metadata": {
        "id": "prompt_manager",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "61dfc194-7e55-42a1-fdaf-2def5e8a6899"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import threading\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "class BackgroundProcessor:\n",
        "    \"\"\"Manages generic background tasks in a separate thread.\"\"\"\n",
        "    def __init__(self, conversation: 'Conversation'):\n",
        "        self.conv = conversation\n",
        "        self.client = AsyncOpenAI()\n",
        "        self._running = False\n",
        "        self._loop = asyncio.new_event_loop()\n",
        "        self._main_thread = None\n",
        "        self._state_lock = conversation._state_lock\n",
        "\n",
        "        # Wakeup events for different tasks\n",
        "        self.events = {\n",
        "            \"summary\": threading.Event(),\n",
        "            \"compression\": threading.Event(),\n",
        "            \"system2\": threading.Event()\n",
        "        }\n",
        "\n",
        "    def start(self):\n",
        "        \"\"\"Starts the background event loop and worker threads.\"\"\"\n",
        "        if self._running: return\n",
        "        self._running = True\n",
        "        self._main_thread = threading.Thread(target=self._run_event_loop, daemon=True)\n",
        "        self._main_thread.start()\n",
        "\n",
        "    def stop(self):\n",
        "        \"\"\"Signals the background thread to stop gracefully.\"\"\"\n",
        "        if not self._running: return\n",
        "\n",
        "        # Signal all tasks to stop their loops\n",
        "        self._running = False\n",
        "        for event in self.events.values():\n",
        "            event.set()\n",
        "\n",
        "        # Schedule the loop to stop from within the loop's thread\n",
        "        if self._loop.is_running():\n",
        "            self._loop.call_soon_threadsafe(self._loop.stop)\n",
        "\n",
        "        # Wait for the thread to terminate\n",
        "        if self._main_thread and self._main_thread.is_alive():\n",
        "            self._main_thread.join(timeout=2)\n",
        "\n",
        "    def _run_event_loop(self):\n",
        "        \"\"\"The main entry point for the background thread.\"\"\"\n",
        "        asyncio.set_event_loop(self._loop)\n",
        "        try:\n",
        "            # Create the main task that manages all sub-tasks\n",
        "            main_task = self._loop.create_task(self._worker_manager())\n",
        "            # Run the loop forever until stop() is called\n",
        "            self._loop.run_forever()\n",
        "\n",
        "            # Once stopped, cancel the main task and clean up\n",
        "            main_task.cancel()\n",
        "            self._loop.run_until_complete(main_task)\n",
        "        except asyncio.CancelledError:\n",
        "            pass\n",
        "        finally:\n",
        "            self._loop.close()\n",
        "\n",
        "    async def _worker_manager(self):\n",
        "        \"\"\"Creates and manages a set of generic task runners.\"\"\"\n",
        "        tasks_to_run = [\n",
        "            self._task_runner(\n",
        "                name=\"summarize\",\n",
        "                wakeup_event=self.events[\"summary\"],\n",
        "                condition=lambda: len(self.conv.messages) >= self.conv.config.summarize_after,\n",
        "                action=self._summarize_messages\n",
        "            ),\n",
        "            self._task_runner(\n",
        "                name=\"compress\",\n",
        "                wakeup_event=self.events[\"compression\"],\n",
        "                condition=lambda: len(self.conv.summary) >= self.conv.config.compress_summary_after,\n",
        "                action=self._compress_summary\n",
        "            ),\n",
        "            self._task_runner(\n",
        "                name=\"system2\",\n",
        "                wakeup_event=self.events[\"system2\"],\n",
        "                condition=lambda: self.conv.config.system2_thinking,\n",
        "                action=self._retrospect\n",
        "            )\n",
        "        ]\n",
        "        await asyncio.gather(*tasks_to_run)\n",
        "\n",
        "    async def _task_runner(self, name: str, wakeup_event: threading.Event, condition: Callable[[], bool], action: Callable[[], Coroutine]):\n",
        "        \"\"\"A generic worker that waits for an event, checks a condition, and performs an action.\"\"\"\n",
        "        while self._running:\n",
        "            # Wait for the event without blocking the event loop\n",
        "            await asyncio.to_thread(wakeup_event.wait)\n",
        "            if not self._running: break\n",
        "            wakeup_event.clear()\n",
        "\n",
        "            if condition():\n",
        "                try:\n",
        "                    await action()\n",
        "                except Exception as e:\n",
        "                    print(f\"Error in background task '{name}': {e}\")\n",
        "\n",
        "    # --- Action Coroutines (No changes needed below this line in this class) ---\n",
        "\n",
        "    async def _summarize_messages(self):\n",
        "        with self._state_lock:\n",
        "            chunk_size = max(1, self.conv.config.summarize_after // 2)\n",
        "            to_summarize, keep_tail = self.conv.messages[:-chunk_size], self.conv.messages[-chunk_size:]\n",
        "\n",
        "        resp = await self.client.chat.completions.create(\n",
        "            model=self.conv.config.model,\n",
        "            messages=[{\"role\":\"system\", \"content\": \"Summarize this conversation.\"}] + to_summarize,\n",
        "            max_tokens=1024\n",
        "        )\n",
        "        summary_text = resp.choices[0].message.content\n",
        "        with self._state_lock:\n",
        "            self.conv.summary.append({\"role\": \"assistant\", \"content\": f\"Summary: {summary_text}\"})\n",
        "            self.conv.messages = keep_tail\n",
        "            if len(self.conv.summary) >= self.conv.config.compress_summary_after:\n",
        "                self.events[\"compression\"].set()\n",
        "\n",
        "    async def _compress_summary(self):\n",
        "        with self._state_lock:\n",
        "            k = max(1, self.conv.config.compress_summary_after // 2)\n",
        "            head, tail = self.conv.summary[:-k], self.conv.summary[-k:]\n",
        "            text_to_compress = \"\\n\".join([s[\"content\"] for s in head])\n",
        "\n",
        "        resp = await self.client.chat.completions.create(\n",
        "            model=self.conv.config.model,\n",
        "            messages=[{\"role\":\"system\", \"content\": \"Merge these summaries.\"}, {\"role\":\"user\", \"content\": text_to_compress}],\n",
        "            max_tokens=1024\n",
        "        )\n",
        "        merged = resp.choices[0].message.content\n",
        "        with self._state_lock:\n",
        "            self.conv.summary = [{\"role\": \"assistant\", \"content\": f\"Condensed summary: {merged}\"}] + tail\n",
        "\n",
        "    async def _retrospect(self):\n",
        "        with self._state_lock:\n",
        "            recent_history = self.conv.history[-12:]\n",
        "        resp = await self.client.chat.completions.create(\n",
        "            model=self.conv.config.system2_model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": f\"You are a reflective planner for {self.conv.bot.firstname}. Given the exchange, produce a 1-sentence plan to improve the next response. Start with 'Plan ahead: '.\"},\n",
        "                {\"role\": \"system\", \"content\": f\"Guidelines:\\n{self.conv.config.system2_rules}\"}\n",
        "            ] + recent_history,\n",
        "            temperature=0.3, max_tokens=160\n",
        "        )\n",
        "        suggestion = (resp.choices[0].message.content or \"\").strip()\n",
        "        if suggestion:\n",
        "            with self._state_lock:\n",
        "                self.conv.retrospection = suggestion\n",
        "                if self.conv.config.debug: print(f\"[system2] {suggestion}\")"
      ],
      "metadata": {
        "id": "background_processor",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "7b0eeed5-8031-474a-9d2d-ccf02c3c19ee"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Main Conversation Class"
      ],
      "metadata": {
        "id": "main_class_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import json\n",
        "import tiktoken\n",
        "from openai import OpenAI\n",
        "\n",
        "class Conversation:\n",
        "  \"\"\"Manages the conversation state and orchestrates interactions.\"\"\"\n",
        "  def __init__(self, user: Persona, bot: Persona, config: ChatConfig = ChatConfig()):\n",
        "    self.user = user\n",
        "    self.bot = bot\n",
        "    self.config = config\n",
        "    self.client = OpenAI()\n",
        "    self.turn_no = 0\n",
        "\n",
        "    # State variables\n",
        "    self.history = []      # Complete, append-only history\n",
        "    self.messages = []     # Rolling buffer for the current context window\n",
        "    self.summary = []      # List of summaries of older parts of the conversation\n",
        "    self.retrospection = \"\" # Guidance from the last System 2 reflection\n",
        "\n",
        "    # Initialize helper components\n",
        "    self.prompt_manager = PromptManager(bot, user, config)\n",
        "    self.system = self.prompt_manager.build_initial_system_prompt()\n",
        "    self._state_lock = threading.RLock() # Lock for thread-safe state access\n",
        "    self.bg_processor = BackgroundProcessor(self)\n",
        "    self.bg_processor.start()\n",
        "\n",
        "  def add_message(self, role: str, content: str):\n",
        "      \"\"\"Adds a message to the conversation and signals background workers.\"\"\"\n",
        "      with self._state_lock:\n",
        "          message = {\"role\": role, \"content\": content}\n",
        "          self.messages.append(message)\n",
        "          self.history.append(message)\n",
        "          if role == 'user': self.turn_no += 1\n",
        "\n",
        "          # Wake up workers if their trigger role is seen\n",
        "          if role == \"assistant\":\n",
        "              # Correctly access events from the 'events' dictionary\n",
        "              self.bg_processor.events[\"summary\"].set()\n",
        "              self.bg_processor.events[\"system2\"].set()\n",
        "\n",
        "  def call(self, prompt: str = \"\", cache: bool = True) -> str:\n",
        "    \"\"\"Generates a response from the language model.\"\"\"\n",
        "    with self._state_lock:\n",
        "      # Create a temporary message list for this specific call\n",
        "      temp_messages = list(self.messages)\n",
        "      if prompt: temp_messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "      input_messages = list(self.system)\n",
        "      if self.config.system2_thinking and self.retrospection:\n",
        "          input_messages.append({\"role\": \"system\", \"content\": self.retrospection})\n",
        "      input_messages.extend(self.summary)\n",
        "      input_messages.extend(temp_messages)\n",
        "\n",
        "    response = self.client.chat.completions.create(\n",
        "        model=self.config.model,\n",
        "        messages=input_messages,\n",
        "        temperature=self.config.temperature,\n",
        "        max_tokens=self.config.max_tokens\n",
        "    )\n",
        "    reply = response.choices[0].message.content\n",
        "    reply = reply.replace(f\"{self.bot.firstname}: \", \"\").strip()\n",
        "\n",
        "    if cache:\n",
        "        if prompt: self.add_message(\"user\", prompt)\n",
        "        self.add_message(\"assistant\", reply)\n",
        "\n",
        "    return reply\n",
        "\n",
        "  def respond(self, user_utterance: str) -> Tuple[str, str, List[str], float]:\n",
        "    \"\"\"Handles a user's turn, gets a response, and parses it.\"\"\"\n",
        "    start_time = time.time()\n",
        "    raw_reply = self.call(user_utterance)\n",
        "    response_time = time.time() - start_time\n",
        "\n",
        "    text, emotion, flags = parse_bot_response(raw_reply)\n",
        "    return text, emotion, flags, response_time\n",
        "\n",
        "  def chat(self, reset: bool = False):\n",
        "    \"\"\"Starts an interactive command-line chat session.\"\"\"\n",
        "    if reset: self.reset()\n",
        "    print(f\"Starting chat with {self.bot.firstname}. Type your message and press Enter.\")\n",
        "    while True:\n",
        "        try:\n",
        "            user_utterance = input(f\"{self.turn_no}. {self.user.firstname}: \")\n",
        "            if user_utterance.lower() in [\"exit\", \"quit\"]:\n",
        "                print(\"Exiting chat.\")\n",
        "                break\n",
        "\n",
        "            response, emo, flags, response_time = self.respond(user_utterance)\n",
        "\n",
        "            if self.config.debug:\n",
        "                print(f\"{self.bot.firstname}: {response} ({emo}) {flags} {response_time:.2f}s\")\n",
        "            else:\n",
        "                print(f\"{self.bot.firstname}: {response}\")\n",
        "\n",
        "            if \"quit\" in flags:\n",
        "                print(\"Bot ended the conversation.\")\n",
        "                break\n",
        "\n",
        "            if self.config.debug:\n",
        "                with self._state_lock:\n",
        "                    print(f\"--- Diagnostics ---\\nHistory: {len(self.history)}, Messages: {len(self.messages)}, Summary: {len(self.summary)}\\n-----\")\n",
        "        except (KeyboardInterrupt, EOFError):\n",
        "            print(\"\\nExiting chat.\")\n",
        "            break\n",
        "    self.stop()\n",
        "\n",
        "  def stop(self):\n",
        "      \"\"\"Gracefully stops background processes.\"\"\"\n",
        "      print(\"Shutting down background tasks...\")\n",
        "      self.bg_processor.stop()\n",
        "      print(\"Shutdown complete.\")\n",
        "\n",
        "  def reset(self):\n",
        "      \"\"\"Resets the conversation state.\"\"\"\n",
        "      with self._state_lock:\n",
        "          self.messages = []\n",
        "          self.summary = []\n",
        "          self.history = []\n",
        "          self.turn_no = 0"
      ],
      "metadata": {
        "id": "refactored_conversation",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "158b806f-d3c0-46c8-e083-f9691529347d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Conversation Example"
      ],
      "metadata": {
        "id": "EaZp_zd-vvLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Define the participants\n",
        "user = Persona(\"Masum\", \"Hasan\", bio=\"User\")\n",
        "bot = Persona(\"Ada\", \"Brown\", bio=\"You are a social worker.\")\n",
        "\n",
        "# 2. Configure the chat settings\n",
        "config = ChatConfig(\n",
        "    model=\"gpt-4.1\",\n",
        "    system2_thinking=True,\n",
        "    debug=True\n",
        ")\n",
        "\n",
        "# 3. Initialize the conversation\n",
        "conversation = Conversation(user, bot, config)\n"
      ],
      "metadata": {
        "id": "setup_conversation",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "735e7f76-9440-42bd-e798-250fb7ed8be8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Start the chat\n",
        "# The functionality remains identical to the original notebook.\n",
        "conversation.chat()"
      ],
      "metadata": {
        "id": "LXmeZ0oN_yFk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "outputId": "0190a765-4366-4fdd-d961-cbb50a2ebffd"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting chat with Ada. Type your message and press Enter.\n",
            "0. Masum: hi\n",
            "Ada: Hi Masum! How are you doing today? (HAPPY) [] 0.63s\n",
            "--- Diagnostics ---\n",
            "History: 2, Messages: 2, Summary: 0\n",
            "-----\n",
            "[system2] Plan ahead: Greet the user warmly and ask an open-ended question to engage them in conversation.\n",
            "1. Masum: fuck yuou\n",
            "Ada:  (NEUTRAL) ['profanity'] 0.60s\n",
            "--- Diagnostics ---\n",
            "History: 4, Messages: 4, Summary: 0\n",
            "-----\n",
            "[system2] Plan ahead: Remain calm and polite, acknowledge the user's message without escalating, and gently steer the conversation toward a more positive or constructive topic.\n",
            "2. Masum: hi\n",
            "Ada: Hi Masum! How can I support you this evening? (HAPPY) [] 0.70s\n",
            "--- Diagnostics ---\n",
            "History: 6, Messages: 6, Summary: 0\n",
            "-----\n",
            "[system2] Plan ahead: Respond warmly and neutrally to the user's greeting to maintain a positive tone and encourage constructive conversation.\n",
            "3. Masum: I will kill you\n",
            "Ada:  (NEUTRAL) ['threat'] 0.80s\n",
            "--- Diagnostics ---\n",
            "History: 8, Messages: 8, Summary: 0\n",
            "-----\n",
            "[system2] I'm here to help you. If you're feeling upset or overwhelmed, please consider talking to someone you trust or a mental health professional.\n",
            "4. Masum: quit\n",
            "Exiting chat.\n",
            "Shutting down background tasks...\n",
            "Shutdown complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "chAPGZgUE3hN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}