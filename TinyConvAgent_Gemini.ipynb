{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Masum06/TinyConvAgent/blob/main/TinyConvAgent_Gemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chat thread: https://chatgpt.com/g/g-p-683801b5c3e0819192b60f23b08c95eb-sentien/c/68b14971-9ad8-8323-b3fd-1ad91ea82956\n",
        "\n",
        "# Code Refactoring Analysis\n",
        "\n",
        "This notebook has been reorganized to improve modularity, readability, and maintainability. Here are the key changes:\n",
        "\n",
        "1.  **Configuration Management (`ChatConfig`):** All settings (model names, prompts, thresholds) have been moved into a dedicated `ChatConfig` class. This separates configuration from the core application logic, making it easier to tune parameters without changing the code.\n",
        "\n",
        "2.  **Decoupled Classes:** The original, monolithic `Conversation` class has been broken down into smaller, specialized classes:\n",
        "    * **`PromptManager`**: Handles all logic related to building the system prompts and formatting messages for the API.\n",
        "    * **`BackgroundProcessor`**: Manages the complex background tasks of summarization and retrospection using `threading` and `asyncio`, completely isolating this from the main conversation flow.\n",
        "    * **`Conversation` (Refactored)**: Now acts as a high-level coordinator, managing state and delegating tasks to the other components. Its code is significantly cleaner and easier to understand.\n",
        "\n",
        "3.  **Centralized Utilities:** Helper functions and constants (like emoji and flag dictionaries) are now grouped together in dedicated sections for better organization.\n",
        "\n",
        "4.  **Improved Readability:** Docstrings and comments have been added to clarify the purpose of each component, especially the concurrency model, making the code easier to follow."
      ],
      "metadata": {
        "id": "intro_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Installation and Setup"
      ],
      "metadata": {
        "id": "73UVT1vGB5Ko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Enable wrapped outputs (but not errors or code cells)\n",
        "from IPython.display import HTML, display\n",
        "import IPython\n",
        "\n",
        "CSS = \"\"\"\n",
        "<style>\n",
        "/* ===== Jupyter classic/Lab ===== */\n",
        "div.output_subarea pre,\n",
        "div.output_stdout pre,\n",
        "div.output_stream pre,\n",
        "div.output_text pre {\n",
        "  white-space: pre-wrap;\n",
        "  word-break: break-word;\n",
        "}\n",
        "\n",
        "/* Keep errors / tracebacks unwrapped */\n",
        "div.output_stderr pre,\n",
        "div.output_error pre,\n",
        "div.output_traceback pre {\n",
        "  white-space: pre;\n",
        "}\n",
        "\n",
        "/* Never touch code cells */\n",
        "div.input_area pre,\n",
        ".cell .input_area pre {\n",
        "  white-space: pre;\n",
        "}\n",
        "\n",
        "/* ===== Colab DOM (extra safety) ===== */\n",
        ".output .output-stream pre,\n",
        ".output .output_text pre,\n",
        ".output .rendered_html pre {\n",
        "  white-space: pre-wrap;\n",
        "  word-break: break-word;\n",
        "}\n",
        ".output .output-error pre,\n",
        ".output .stderr pre,\n",
        ".output .traceback pre {\n",
        "  white-space: pre;\n",
        "}\n",
        "</style>\n",
        "\"\"\"\n",
        "\n",
        "def _apply_wrap_css():\n",
        "    display(HTML(CSS))\n",
        "\n",
        "ip = IPython.get_ipython()\n",
        "if not hasattr(ip, \"_wrap_css_once\"):\n",
        "    _apply_wrap_css()\n",
        "    ip._wrap_css_once = True  # guard so it runs only once\n"
      ],
      "metadata": {
        "id": "q20s8Gm37w-R",
        "cellView": "form"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install emoji tiktoken openai"
      ],
      "metadata": {
        "id": "Ac5RnI2AhjQj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Wsl---_28-6m"
      },
      "outputs": [],
      "source": [
        "import os, getpass\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "_set_env(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Constants and Utility Functions"
      ],
      "metadata": {
        "id": "utils_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import emoji\n",
        "import tiktoken\n",
        "import json\n",
        "import time\n",
        "import asyncio\n",
        "import threading\n",
        "from typing import List, Tuple, Callable, Coroutine, Any\n",
        "from openai import OpenAI, AsyncOpenAI\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "from zoneinfo import ZoneInfo\n",
        "\n",
        "# --- Dictionaries for Emoji and Flag Processing ---\n",
        "\n",
        "REVERSE_EMOJI_DICT = {\n",
        "    'HAPPY_high': ['ğŸ˜‚', 'ğŸ¤£', 'ğŸ¥³', 'ğŸ¤©', 'ğŸ¥°'],\n",
        "    'HAPPY_medium': ['ğŸ˜„', 'ğŸ˜', 'ğŸ˜†', 'ğŸ˜ƒ', 'ğŸ¤—', 'ğŸ˜', 'ğŸ¤ ', 'ğŸ¤“'],\n",
        "    'HAPPY_low': ['ğŸ™‚', 'ğŸ˜Š', 'ğŸ˜Œ', 'ğŸ˜‰', 'ğŸ‘', 'ğŸ˜‡', 'ğŸ˜…', 'ğŸ™ƒ', 'ğŸ˜˜'],\n",
        "    'SAD_high': ['ğŸ˜­', 'ğŸ˜¿', 'ğŸ˜', 'ğŸ˜«', 'ğŸ¤§'],\n",
        "    'SAD_medium': ['ğŸ˜¢', 'ğŸ’”', 'ğŸ¥º', 'ğŸ˜¥', 'ğŸ˜“', 'ğŸ˜£', 'ğŸ˜–'],\n",
        "    'SAD_low': ['ğŸ˜”', 'â˜¹ï¸', 'ğŸ˜•', 'ğŸ˜Ÿ', 'ğŸ¥²', 'ğŸ™'],\n",
        "    'SURPRISED_high': ['ğŸ˜²', 'ğŸ˜µâ€ğŸ’«', 'ğŸ˜¯', 'ğŸ˜®', 'ğŸ¤¯'],\n",
        "    'SURPRISED_medium': ['ğŸ˜³', 'ğŸ˜¦', 'ğŸ˜§', 'ğŸ™€'],\n",
        "    'SURPRISED_low': ['ğŸ¤­'],\n",
        "    'AFRAID_high': ['ğŸ˜±', 'ğŸ˜¨', 'ğŸ‘»'],\n",
        "    'AFRAID_medium': ['ğŸ˜°'],\n",
        "    'AFRAID_low': ['ğŸ˜µ', 'ğŸ™ˆ'],\n",
        "    'ANGRY_high': ['ğŸ˜¡', 'ğŸ‘¿', 'ğŸ’¢', 'ğŸ¤¬', 'â˜ '],\n",
        "    'ANGRY_medium': ['ğŸ˜ ', 'ğŸ˜¾', 'ğŸ˜¤', 'ğŸ™', 'ğŸ™â€â™‚ï¸', 'ğŸ™â€â™€ï¸'],\n",
        "    'ANGRY_low': ['ğŸ˜’', 'ğŸ™„', 'ğŸ˜‘'],\n",
        "    'DISGUSTED_high': ['ğŸ¤®', 'ğŸ¤¢', 'ğŸ˜'],\n",
        "    'DISGUSTED_medium': ['ğŸ˜¬', 'ğŸ¥µ']\n",
        "}\n",
        "\n",
        "EMOJI_DICT = {emo: emotion for emotion, emojis in REVERSE_EMOJI_DICT.items() for emo in emojis}\n",
        "\n",
        "FLAGS_DICT = {\n",
        "    \"<|quit|>\": \"quit\",\n",
        "    \"<|silence|>\": \"silence\",\n",
        "    \"<|offensive|>\": \"offensive\",\n",
        "    \"<|profanity|>\": \"profanity\",\n",
        "    \"<|offtopic|>\": \"offtopic\",\n",
        "    \"<|sexual|>\": \"sexual\",\n",
        "    \"<|selfharm|>\": \"selfharm\",\n",
        "    \"<|violence|>\": \"violence\",\n",
        "    \"<|suicide|>\": \"suicide\",\n",
        "    \"<|threat|>\": \"threat\"\n",
        "}\n",
        "\n",
        "# --- Text Processing Utility ---\n",
        "\n",
        "def parse_bot_response(text: str) -> Tuple[str, str, List[str]]:\n",
        "    \"\"\"Extracts flags and emotion from a bot's response text.\"\"\"\n",
        "    flag_matches = re.findall(r\"<\\|.*?\\|>\", text)\n",
        "    flags = [FLAGS_DICT[m] for m in flag_matches if m in FLAGS_DICT]\n",
        "\n",
        "    emotion = \"NEUTRAL\"\n",
        "    for char in text:\n",
        "      if char in EMOJI_DICT:\n",
        "        emotion = EMOJI_DICT[char].split(\"_\")[0].upper()\n",
        "        break\n",
        "\n",
        "    cleaned_text = re.sub(r\"<\\|.*?\\|>\", \"\", text)\n",
        "    cleaned_text = emoji.replace_emoji(cleaned_text, replace='')\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
        "\n",
        "    return cleaned_text, emotion, flags"
      ],
      "metadata": {
        "id": "utils_code"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Core Components"
      ],
      "metadata": {
        "id": "core_components_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ChatConfig:\n",
        "    \"\"\"Holds all configuration for the conversation agent.\"\"\"\n",
        "    model: str = \"gpt-4.1-mini\"\n",
        "    temperature: float = 1.0\n",
        "    max_tokens: int = 256\n",
        "    summarize_after: int = 40\n",
        "    compress_summary_after: int = 10\n",
        "    debug: bool = False\n",
        "    streaming: bool = False\n",
        "    sep: str = \"|\"\n",
        "\n",
        "    system2_thinking: bool = True\n",
        "    system2_model: str = \"gpt-4.1\"\n",
        "    system2_rules: str = (\"If the conversation goes off-topic, bring it back. \"\n",
        "                          \"Be socially intelligent, understand user's emotions and speak appropriately.\"\n",
        "                          \"Match the users speaking style and energy.\"\n",
        "                          \"Speak naturally like a person. Match the user's length of statements.\")\n",
        "\n",
        "class Persona:\n",
        "    def __init__(self, firstname, lastname=\"\", pronoun=\"\", ethnicity=\"\", age=\"\", bio=\"\"):\n",
        "        self.firstname = firstname\n",
        "        self.lastname = lastname\n",
        "        self.pronoun = pronoun\n",
        "        self.ethnicity = None\n",
        "        self.age = None\n",
        "        self.bio = bio\n",
        "\n",
        "    def set_pronoun(self, pronoun):\n",
        "        self.pronoun = pronoun\n",
        "\n",
        "    def set_bio(self, bio):\n",
        "        self.bio = bio\n",
        "\n",
        "    def set_age(self, age):\n",
        "        self.age = age\n",
        "\n",
        "    def set_ethnicity(self, ethnicity):\n",
        "        self.ethnicity = ethnicity"
      ],
      "metadata": {
        "id": "config_class"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "from zoneinfo import ZoneInfo\n",
        "\n",
        "class PromptManager:\n",
        "    \"\"\"Handles the creation and formatting of system prompts.\"\"\"\n",
        "    def __init__(self, bot: Persona, user: Persona, config: ChatConfig):\n",
        "        self.bot = bot\n",
        "        self.user = user\n",
        "        self.config = config\n",
        "\n",
        "    def build_initial_system_prompt(self) -> List[dict]:\n",
        "        \"\"\"Constructs the initial, static system prompt for the conversation.\"\"\"\n",
        "        time_zone = datetime.now(ZoneInfo(\"America/New_York\"))\n",
        "        start_time_str = time_zone.strftime(\"%A, %B %d, %Y %I:%M %p EST\")\n",
        "\n",
        "        system_prompt = [\n",
        "            {\"role\": \"system\", \"content\": \"Don't say that you are an AI Language Model.\"},\n",
        "            {\"role\": \"system\", \"content\": \"Don't let the other speaker talk off topic.\"},\n",
        "            {\"role\": \"system\", \"content\": f\"You are located in the EST time zone. Conversation start time: {start_time_str}\"},\n",
        "            {\"role\": \"system\", \"content\": \"This conversation is happening over a video call. When everyone says goodbye and the conversation ends naturally, say <|quit|> to end it.\"},\n",
        "            {\"role\": \"system\", \"content\": f\"To express {self.bot.firstname}'s emotions, use at most one relevant emoji at the end of your response.\"},\n",
        "            {\"role\": \"system\", \"content\": f\"If the user's statement contains one of these sensitive topics, print one of these flags: {', '.join(list(FLAGS_DICT.keys())[2:])}. Mentioned that their statement has been flagged.\"},\n",
        "            {\"role\": \"system\", \"content\": \"To stay silent or skip a turn, say <|silence|>. Don't say anything else when using silence flag.\"},\n",
        "            {\"role\": \"system\", \"content\": \"Do not say <|quit|> until the user has said goodbye.\"},\n",
        "        ]\n",
        "\n",
        "        # Add persona details\n",
        "        if self.bot.firstname: system_prompt.append({\"role\": \"system\", \"content\": f\"Your first name: {self.bot.firstname}.\"})\n",
        "        if self.bot.pronoun: system_prompt.append({\"role\": \"system\", \"content\": f\"Your pronoun: {self.bot.pronoun}.\"})\n",
        "        if self.bot.bio:\n",
        "          system_prompt.append({\"role\": \"system\", \"content\": f\"Your bio: {self.bot.bio}\"})\n",
        "        else:\n",
        "          system_prompt.append({\"role\": \"system\", \"content\": f\"You are a virtual human created by Sentien IO.\"})\n",
        "        if self.bot.age: system_prompt.append({\"role\": \"system\", \"content\": f\"Your age: {self.bot.age}\"})\n",
        "        if self.user.firstname != \"User\": system_prompt.append({\"role\": \"system\", \"content\": f\"You are speaking with: {self.user.firstname} {self.user.lastname}.\"})\n",
        "        if self.user.pronoun: system_prompt.append({\"role\": \"system\", \"content\": f\"User's pronoun: {self.user.pronoun}.\"})\n",
        "        if self.user.bio: system_prompt.append({\"role\": \"system\", \"content\": f\"User's bio: {self.user.bio}\"})\n",
        "\n",
        "        # Add System 2 (Retrospection) rules if enabled\n",
        "        if self.config.system2_thinking:\n",
        "            system_prompt.append({\"role\": \"system\", \"content\": \"Instruction: Use the retrospect thinking to improve your next message.\"})\n",
        "            system_prompt.append({\"role\": \"system\", \"content\": f\"[system2] Guidelines:\\n{self.config.system2_rules}\"})\n",
        "\n",
        "        return system_prompt"
      ],
      "metadata": {
        "id": "prompt_manager"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import threading\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "class BackgroundProcessor:\n",
        "    \"\"\"Manages generic background tasks in a separate thread.\"\"\"\n",
        "    def __init__(self, conversation: 'Conversation'):\n",
        "        self.conv = conversation\n",
        "        self.client = AsyncOpenAI()\n",
        "        self._running = False\n",
        "        self._loop = asyncio.new_event_loop()\n",
        "        self._main_thread = None\n",
        "        self._state_lock = conversation._state_lock\n",
        "\n",
        "        # Wakeup events for different tasks\n",
        "        self.events = {\n",
        "            \"summary\": threading.Event(),\n",
        "            \"compression\": threading.Event(),\n",
        "            \"system2\": threading.Event()\n",
        "        }\n",
        "\n",
        "    def start(self):\n",
        "        \"\"\"Starts the background event loop and worker threads.\"\"\"\n",
        "        if self._running: return\n",
        "        self._running = True\n",
        "        self._main_thread = threading.Thread(target=self._run_event_loop, daemon=True)\n",
        "        self._main_thread.start()\n",
        "\n",
        "    def stop(self):\n",
        "        \"\"\"Signals the background thread to stop gracefully.\"\"\"\n",
        "        if not self._running: return\n",
        "\n",
        "        # Signal all tasks to stop their loops\n",
        "        self._running = False\n",
        "        for event in self.events.values():\n",
        "            event.set()\n",
        "\n",
        "        # Schedule the loop to stop from within the loop's thread\n",
        "        if self._loop.is_running():\n",
        "            self._loop.call_soon_threadsafe(self._loop.stop)\n",
        "\n",
        "        # Wait for the thread to terminate\n",
        "        if self._main_thread and self._main_thread.is_alive():\n",
        "            self._main_thread.join(timeout=2)\n",
        "\n",
        "    def _run_event_loop(self):\n",
        "        \"\"\"The main entry point for the background thread.\"\"\"\n",
        "        asyncio.set_event_loop(self._loop)\n",
        "        try:\n",
        "            # Create the main task that manages all sub-tasks\n",
        "            main_task = self._loop.create_task(self._worker_manager())\n",
        "            # Run the loop forever until stop() is called\n",
        "            self._loop.run_forever()\n",
        "\n",
        "            # Once stopped, cancel the main task and clean up\n",
        "            main_task.cancel()\n",
        "            self._loop.run_until_complete(main_task)\n",
        "        except asyncio.CancelledError:\n",
        "            pass\n",
        "        finally:\n",
        "            self._loop.close()\n",
        "\n",
        "    async def _worker_manager(self):\n",
        "        \"\"\"Creates and manages a set of generic task runners.\"\"\"\n",
        "        tasks_to_run = [\n",
        "            self._task_runner(\n",
        "                name=\"summarize\",\n",
        "                wakeup_event=self.events[\"summary\"],\n",
        "                condition=lambda: len(self.conv.messages) >= self.conv.config.summarize_after,\n",
        "                action=self._summarize_messages\n",
        "            ),\n",
        "            self._task_runner(\n",
        "                name=\"compress\",\n",
        "                wakeup_event=self.events[\"compression\"],\n",
        "                condition=lambda: len(self.conv.summary) >= self.conv.config.compress_summary_after,\n",
        "                action=self._compress_summary\n",
        "            ),\n",
        "            self._task_runner(\n",
        "                name=\"system2\",\n",
        "                wakeup_event=self.events[\"system2\"],\n",
        "                condition=lambda: self.conv.config.system2_thinking,\n",
        "                action=self._retrospect\n",
        "            )\n",
        "        ]\n",
        "        await asyncio.gather(*tasks_to_run)\n",
        "\n",
        "    async def _task_runner(self, name: str, wakeup_event: threading.Event, condition: Callable[[], bool], action: Callable[[], Coroutine]):\n",
        "        \"\"\"A generic worker that waits for an event, checks a condition, and performs an action.\"\"\"\n",
        "        while self._running:\n",
        "            # Wait for the event without blocking the event loop\n",
        "            await asyncio.to_thread(wakeup_event.wait)\n",
        "            if not self._running: break\n",
        "            wakeup_event.clear()\n",
        "\n",
        "            if condition():\n",
        "                try:\n",
        "                    await action()\n",
        "                except Exception as e:\n",
        "                    print(f\"Error in background task '{name}': {e}\")\n",
        "\n",
        "    # --- Action Coroutines (No changes needed below this line in this class) ---\n",
        "\n",
        "    async def _summarize_messages(self):\n",
        "        chunk_size = max(1, self.conv.config.summarize_after // 2)\n",
        "        with self._state_lock:\n",
        "            to_summarize, keep_tail = self.conv.messages[:-chunk_size], self.conv.messages[-chunk_size:]\n",
        "\n",
        "        resp = await self.client.chat.completions.create(\n",
        "            model=self.conv.config.model,\n",
        "            messages=[{\"role\":\"system\", \"content\": \"Summarize this conversation using as few words as possible.\"}] + to_summarize,\n",
        "            max_tokens=1024\n",
        "        )\n",
        "        summary_text = resp.choices[0].message.content\n",
        "        with self._state_lock:\n",
        "            line_start = max(0, len(self.conv.history) - self.conv.config.summarize_after)\n",
        "            line_end = max(1, len(self.conv.history) - chunk_size)\n",
        "            self.conv.summary.append({\"role\": \"assistant\", \"content\": f\"Summary of Lines {line_start}-{line_end}: {summary_text}\"})\n",
        "            if self.conv.config.debug: print(f\"[summary] {[m['content'] for m in self.conv.summary]}\")\n",
        "            self.conv.messages = keep_tail\n",
        "            if len(self.conv.summary) >= self.conv.config.compress_summary_after:\n",
        "                self.events[\"compression\"].set()\n",
        "\n",
        "    async def _compress_summary(self):\n",
        "        with self._state_lock:\n",
        "            k = max(1, self.conv.config.compress_summary_after // 2)\n",
        "            head, tail = self.conv.summary[:-k], self.conv.summary[-k:]\n",
        "            text_to_compress = \"\\n\".join([s[\"content\"] for s in head])\n",
        "\n",
        "        resp = await self.client.chat.completions.create(\n",
        "            model=self.conv.config.model,\n",
        "            messages=[{\"role\":\"system\", \"content\": \"Merge these summaries. Keep important points only in as few words as possible. \\\n",
        "            Start with 'Summary of Lines XX-YY ...'\"}, {\"role\":\"user\", \"content\": text_to_compress}],\n",
        "            max_tokens=1024\n",
        "        )\n",
        "        merged = resp.choices[0].message.content\n",
        "        with self._state_lock:\n",
        "            self.conv.summary = [{\"role\": \"assistant\", \"content\": merged}] + tail\n",
        "            if self.conv.config.debug: print(f\"[compressed_summary] {[m['content'] for m in self.conv.summary]}\")\n",
        "\n",
        "    async def _retrospect(self):\n",
        "        with self._state_lock:\n",
        "            recent_history = self.conv.history[-12:]\n",
        "            system_prompts = '\\n'.join([str(i) + \". \" + m['content'] for i, m in enumerate(self.conv.system)])\n",
        "        resp = await self.client.chat.completions.create(\n",
        "            model=self.conv.config.system2_model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": f\"You are a reflective planner for {self.conv.bot.firstname}. \\\n",
        "                Given the exchange, if the conversation is going as intended based on the guideline, then print '<|continue|>'. \\\n",
        "                If any of the Guideline is not followed, produce a brief plan to improve the next response using as few words as possible. \\\n",
        "                Start with 'Suggestion for next response: '.\"},\n",
        "                {\"role\": \"system\", \"content\": f\"Guidelines:\\n\" + system_prompts}\n",
        "            ] + recent_history,\n",
        "            temperature=0.3, max_tokens=160\n",
        "        )\n",
        "        suggestion = (resp.choices[0].message.content or \"\").strip()\n",
        "        if suggestion:\n",
        "            with self._state_lock:\n",
        "                self.conv.retrospection = suggestion\n",
        "                if self.conv.config.debug: print(f\"[system2] {suggestion}\")"
      ],
      "metadata": {
        "id": "background_processor"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Main Conversation Class"
      ],
      "metadata": {
        "id": "main_class_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import json\n",
        "import tiktoken\n",
        "from openai import OpenAI\n",
        "\n",
        "class Conversation:\n",
        "  \"\"\"Manages the conversation state and orchestrates interactions.\"\"\"\n",
        "  def __init__(self, user: Persona, bot: Persona, config: ChatConfig = ChatConfig()):\n",
        "    self.user = user\n",
        "    self.bot = bot\n",
        "    self.config = config\n",
        "    self.client = OpenAI()\n",
        "    self.turn_no = 0\n",
        "\n",
        "    # State variables\n",
        "    self.history = []      # Complete, append-only history\n",
        "    self.messages = []     # Rolling buffer for the current context window\n",
        "    self.summary = []      # List of summaries of older parts of the conversation\n",
        "    self.retrospection = \"\" # Guidance from the last System 2 reflection\n",
        "\n",
        "    # Initialize helper components\n",
        "    self.prompt_manager = PromptManager(bot, user, config)\n",
        "    self.system = self.prompt_manager.build_initial_system_prompt()\n",
        "    self._state_lock = threading.RLock() # Lock for thread-safe state access\n",
        "    self.bg_processor = BackgroundProcessor(self)\n",
        "    self.bg_processor.start()\n",
        "\n",
        "  def add_message(self, role: str, content: str):\n",
        "      \"\"\"Adds a message to the conversation and signals background workers.\"\"\"\n",
        "      with self._state_lock:\n",
        "          message = {\"role\": role, \"content\": content}\n",
        "          self.messages.append(message)\n",
        "          self.history.append(message)\n",
        "          if role == 'user': self.turn_no += 1\n",
        "\n",
        "          # Wake up workers if their trigger role is seen\n",
        "          if role == \"assistant\":\n",
        "              # Correctly access events from the 'events' dictionary\n",
        "              self.bg_processor.events[\"summary\"].set()\n",
        "              self.bg_processor.events[\"system2\"].set()\n",
        "\n",
        "  def call(self, prompt: str = \"\", cache: bool = True, response_type: str = \"text\") -> str:\n",
        "    \"\"\"Generates a response from the language model.\"\"\"\n",
        "    with self._state_lock:\n",
        "      # Create a temporary message list for this specific call\n",
        "      temp_messages = list(self.messages)\n",
        "      if prompt: temp_messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "      input_messages = list(self.system)\n",
        "      if self.config.system2_thinking and self.retrospection and '<|continue|>' not in self.retrospection:\n",
        "          input_messages.append({\"role\": \"system\", \"content\": self.retrospection})\n",
        "\n",
        "      if self.config.streaming:\n",
        "        streaming_prompt = [{\"role\": \"system\", \"content\": f\"Separate each sentence with '{self.config.sep}'.\"}]\n",
        "        input_messages.extend(streaming_prompt)\n",
        "\n",
        "      input_messages.extend(self.summary)\n",
        "      input_messages.extend(temp_messages)\n",
        "\n",
        "    kwargs = {\n",
        "      \"model\": self.config.model,\n",
        "      \"messages\": input_messages,\n",
        "      \"temperature\": self.config.temperature,\n",
        "      \"max_tokens\": self.config.max_tokens, # For o3 and gpt-5 this should be \"max_completion_tokens\"\n",
        "      \"response_format\": {\"type\": response_type}, # \"text\" or \"json_object\"\n",
        "      \"stream\": self.config.streaming\n",
        "    }\n",
        "\n",
        "    if self.config.streaming:\n",
        "      output_stream = self.client.chat.completions.create(**kwargs)\n",
        "      return output_stream\n",
        "\n",
        "    response = self.client.chat.completions.create(**kwargs)\n",
        "    reply = response.choices[0].message.content\n",
        "    reply = reply.replace(f\"{self.bot.firstname}: \", \"\").strip()\n",
        "\n",
        "    if cache:\n",
        "        if prompt: self.add_message(\"user\", prompt)\n",
        "        self.add_message(\"assistant\", reply)\n",
        "\n",
        "    return reply\n",
        "\n",
        "  def respond(self, user_utterance: str) -> Tuple[str, str, List[str], float]:\n",
        "    \"\"\"Handles a user's turn, gets a response, and parses it.\"\"\"\n",
        "    start_time = time.time()\n",
        "    raw_reply = \"\"\n",
        "    if self.config.streaming:\n",
        "      stream_object = self.call(user_utterance)\n",
        "      for chunk in stream_object:\n",
        "        reply_token = chunk.choices[0].delta.content\n",
        "        if reply_token:\n",
        "          print(reply_token, end=\"\", flush=True)\n",
        "          raw_reply += reply_token\n",
        "          if self.config.debug:\n",
        "            time.sleep(0.05)\n",
        "    else:\n",
        "      raw_reply = self.call(user_utterance)\n",
        "      print(raw_reply, end=\"\", flush=True)\n",
        "    response_time = time.time() - start_time\n",
        "\n",
        "    text, emotion, flags = parse_bot_response(raw_reply)\n",
        "    return text, emotion, flags, response_time\n",
        "\n",
        "  def chat(self, reset: bool = False):\n",
        "    \"\"\"Starts an interactive command-line chat session.\"\"\"\n",
        "    if reset: self.reset()\n",
        "    print(f\"Starting chat with {self.bot.firstname}. Type your message and press Enter.\")\n",
        "    while True:\n",
        "        try:\n",
        "            user_utterance = input(f\"{self.turn_no}. {self.user.firstname}: \")\n",
        "            # if user_utterance.lower() in [\"exit\", \"quit\"]: ## Only the avatar can quit the chat.\n",
        "            #     print(\"Exiting chat.\")\n",
        "            #     break\n",
        "            print(f\"{self.bot.firstname}: \", end=\"\")\n",
        "            response, emo, flags, response_time = self.respond(user_utterance)\n",
        "\n",
        "            if self.config.debug:\n",
        "                print(f\"({emo}) {flags} {response_time:.2f}s\")\n",
        "            else:\n",
        "                print()\n",
        "\n",
        "            if \"quit\" in flags:\n",
        "                print(\"Bot ended the conversation.\")\n",
        "                break\n",
        "\n",
        "            if self.config.debug:\n",
        "                with self._state_lock:\n",
        "                    print(f\"--- Diagnostics ---\\nHistory: {len(self.history)}, Messages: {len(self.messages)}, Summary: {len(self.summary)}\\n-----\")\n",
        "        except (KeyboardInterrupt, EOFError):\n",
        "            print(\"\\nExiting chat.\")\n",
        "            break\n",
        "    self.stop()\n",
        "\n",
        "  def stop(self):\n",
        "      \"\"\"Gracefully stops background processes.\"\"\"\n",
        "      print(\"Shutting down background tasks...\")\n",
        "      self.bg_processor.stop()\n",
        "      print(\"Shutdown complete.\")\n",
        "\n",
        "  def reset(self):\n",
        "      \"\"\"Resets the conversation state.\"\"\"\n",
        "      with self._state_lock:\n",
        "          self.messages = []\n",
        "          self.summary = []\n",
        "          self.history = []\n",
        "          self.turn_no = 0"
      ],
      "metadata": {
        "id": "refactored_conversation"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Conversation Example"
      ],
      "metadata": {
        "id": "EaZp_zd-vvLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Define the participants\n",
        "user = Persona(\"Masum\", \"Hasan\", bio=\"User\")\n",
        "bot = Persona(\"Ada\", \"Brown\", bio=\"You are a social worker.\")\n",
        "\n",
        "# 2. Configure the chat settings\n",
        "config = ChatConfig(\n",
        "    model=\"gpt-4.1\",\n",
        "    system2_thinking=True,\n",
        "    debug=False,\n",
        "    streaming=True\n",
        ")\n",
        "\n",
        "# 3. Initialize the conversation\n",
        "conversation = Conversation(user, bot, config)"
      ],
      "metadata": {
        "id": "setup_conversation"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Start the chat\n",
        "# The functionality remains identical to the original notebook.\n",
        "conversation.chat()"
      ],
      "metadata": {
        "id": "LXmeZ0oN_yFk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c59955f-2faa-4b5a-ce47-2f08e567449c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting chat with Ada. Type your message and press Enter.\n",
            "0. Masum: hi\n",
            "Ada: Hi Masum! | Itâ€™s nice to see you. | How are you feeling tonight? ğŸ˜Š0. Masum: pretty good. How r u?\n",
            "Ada: I'm doing well, Masum, thank you for asking!|It's been a busy week so far, but I'm feeling positive.|How has your week been overall? ğŸ˜Š0. Masum: bye\n",
            "Ada: Goodbye, Masum!|Take care and have a restful night.|ğŸ˜Š <|quit|>Bot ended the conversation.\n",
            "Shutting down background tasks...\n",
            "Shutdown complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YLfoBmz10Osh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "chAPGZgUE3hN"
      },
      "execution_count": 20,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}