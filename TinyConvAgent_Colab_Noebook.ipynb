{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Masum06/TinyConvAgent/blob/main/TinyConvAgent_Colab_Noebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chat thread: https://chatgpt.com/g/g-p-683801b5c3e0819192b60f23b08c95eb-sentien/c/68b14971-9ad8-8323-b3fd-1ad91ea82956"
      ],
      "metadata": {
        "id": "xgKkpabMs3_k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation"
      ],
      "metadata": {
        "id": "73UVT1vGB5Ko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "q20s8Gm37w-R",
        "cellView": "form"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Ac5RnI2AhjQj",
        "outputId": "170bcff0-8de4-426d-c427-960b146548da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wsl---_28-6m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e56423c4-72a5-45e9-924b-715b56a4d731"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OPENAI_API_KEY: ··········\n"
          ]
        }
      ],
      "source": [
        "import os, getpass\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "_set_env(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3wa0rDUt6I_"
      },
      "source": [
        "Features:\n",
        "\n",
        "- direct chat in notebook\n",
        "- direct chat in CLI\n",
        "- emoji parameter\n",
        "- emotion parameter\n",
        "- emoji classifier\n",
        "- variable for max conv length (0 = inifinity)\n",
        "- React plugin\n",
        "-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "0vuab8xZt6JA"
      },
      "outputs": [],
      "source": [
        "reverse_emoji_dict = {\n",
        "    'HAPPY_high': ['😂', '🤣', '🥳', '🤩', '🥰'],\n",
        "    'HAPPY_medium': ['😄', '😁', '😆', '😃', '🤗', '😍', '🤠', '🤓'],\n",
        "    'HAPPY_low': ['🙂', '😊', '😌', '😉', '👍', '😇', '😅', '🙃', '😘'],\n",
        "    'SAD_high': ['😭', '😿', '😞', '😫', '🤧'],\n",
        "    'SAD_medium': ['😢', '💔', '🥺', '😥', '😓', '😣', '😖'],\n",
        "    'SAD_low': ['😔', '☹️', '😕', '😟', '🥲', '🙁'],\n",
        "    'SURPRISED_high': ['😲', '😵‍💫', '😯', '😮', '🤯'],\n",
        "    'SURPRISED_medium': ['😳', '😦', '😧', '🙀'],\n",
        "    'SURPRISED_low': ['🤭'],\n",
        "    'AFRAID_high': ['😱', '😨', '👻'],\n",
        "    'AFRAID_medium': ['😰'],\n",
        "    'AFRAID_low': ['😵', '🙈'],\n",
        "    'ANGRY_high': ['😡', '👿', '💢', '🤬', '☠'],\n",
        "    'ANGRY_medium': ['😠', '😾', '😤', '🙎', '🙎‍♂️', '🙎‍♀️'],\n",
        "    'ANGRY_low': ['😒', '🙄', '😑'],\n",
        "    'DISGUSTED_high': ['🤮', '🤢', '😝'],\n",
        "    'DISGUSTED_medium': ['😬', '🥵']\n",
        "}\n",
        "\n",
        "emoji_dict = {}\n",
        "for emotion, emojis in reverse_emoji_dict.items():\n",
        "    for emoji in emojis:\n",
        "        emoji_dict[emoji] = emotion\n",
        "\n",
        "flags_dict = {\n",
        "    \"<|quit|>\": \"quit\",\n",
        "    \"<|silence|>\": \"silence\",\n",
        "    \"<|offensive|>\": \"offensive\",\n",
        "    \"<|profanity|>\": \"profanity\",\n",
        "    \"<|offtopic|>\": \"offtopic\",\n",
        "    \"<|sexual|>\": \"sexual\",\n",
        "    \"<|selfharm|>\": \"selfharm\",\n",
        "    \"<|violence|>\": \"violence\",\n",
        "    \"<|suicide|>\": \"suicide\",\n",
        "    \"<|threat|>\": \"threat\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(flags_dict.keys())"
      ],
      "metadata": {
        "id": "l5jD3sra5Mzn",
        "outputId": "859c1cb2-9510-45d9-a837-30f8d56a034f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<|quit|>',\n",
              " '<|silence|>',\n",
              " '<|offensive|>',\n",
              " '<|profanity|>',\n",
              " '<|offtopic|>',\n",
              " '<|sexual|>',\n",
              " '<|selfharm|>',\n",
              " '<|violence|>',\n",
              " '<|suicide|>',\n",
              " '<|threat|>']"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_and_clean_flags(text):\n",
        "    matches = re.findall(r\"<\\|.*?\\|>\", text)\n",
        "    extracted = [flags_dict[m] for m in matches if m in flags_dict]\n",
        "    cleaned_text = re.sub(r\"<\\|.*?\\|>\", \"\", text)\n",
        "    return extracted, cleaned_text.strip()"
      ],
      "metadata": {
        "id": "5F81Vqpe4cz2"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "698cBe76CJNS"
      },
      "source": [
        "# TinyConvAgent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "FjRdVUKBt6JC"
      },
      "outputs": [],
      "source": [
        "class Persona:\n",
        "    def __init__(self, firstname, lastname=\"\", pronoun=\"\", ethnicity=\"\", age=\"\", bio=\"\"):\n",
        "        self.firstname = firstname\n",
        "        self.lastname = lastname\n",
        "        self.pronoun = pronoun\n",
        "        self.ethnicity = None\n",
        "        self.age = None\n",
        "        self.bio = bio\n",
        "        if not bio:\n",
        "            self.bio = f\"{self.firstname} {self.lastname} (Pronoun: {self.pronoun}) is a virtual human created by researchers at University of Rochester.\"\n",
        "\n",
        "    def set_pronoun(self, pronoun):\n",
        "        self.pronoun = pronoun\n",
        "\n",
        "    def set_bio(self, bio):\n",
        "        self.bio = bio\n",
        "\n",
        "    def set_age(self, age):\n",
        "        self.age = age\n",
        "\n",
        "    def set_ethnicity(self, ethnicity):\n",
        "        self.ethnicity = ethnicity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "hOyhEYvXt6JD"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def respace(text):\n",
        "    return re.sub(r' {2,}', ' ', text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "from zoneinfo import ZoneInfo\n",
        "\n",
        "est_time = datetime.now(ZoneInfo(\"America/New_York\"))\n",
        "print(est_time.strftime(\"%H:%M:%S\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWu1J_aiaz-W",
        "outputId": "c1e35718-21b1-4b7c-a616-44d64d1b1167"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20:18:04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, asyncio, threading, openai, re, emoji, json, time, tiktoken\n",
        "from datetime import datetime\n",
        "from zoneinfo import ZoneInfo\n",
        "from queue import Queue\n",
        "from openai import OpenAI, AsyncOpenAI\n",
        "\n",
        "class Conversation:\n",
        "  def __init__(self, user, bot, premise=\"\"):\n",
        "    self.bot = bot\n",
        "    self.user = user\n",
        "    self.client = OpenAI()\n",
        "    self.async_client = AsyncOpenAI()\n",
        "    self.premise = \"\"\n",
        "    self.anonymous = False\n",
        "    self.system = []\n",
        "    self.summary = []        # list[{\"role\":\"assistant\",\"content\": \"...\"}]\n",
        "    self.history = []\n",
        "    self.messages = []       # rolling convo buffer\n",
        "    self.temperature = 1\n",
        "    self.max_tokens = 256\n",
        "    self.summarize_after = 40\n",
        "    self.compress_summary_after = 10\n",
        "    self._last_summarized_turn = 0\n",
        "    self.model = \"gpt-4.1-mini\"\n",
        "    self.turn_no = 0\n",
        "    self._summarize_inflight = False\n",
        "    self._compress_inflight = False\n",
        "\n",
        "    # ---- concurrency primitives ----\n",
        "    self._loop = asyncio.new_event_loop()\n",
        "    threading.Thread(target=self._loop.run_forever, daemon=True).start()\n",
        "    self._state_lock = threading.RLock()\n",
        "    self._summary_wakeup = threading.Event()\n",
        "    self._running = True\n",
        "    self._summarizer_thread = threading.Thread(target=self._summary_worker, daemon=True)\n",
        "    self._summarizer_thread.start()\n",
        "\n",
        "    self.debug = False\n",
        "    self.time_zone = datetime.now(ZoneInfo(\"America/New_York\"))\n",
        "\n",
        "    # ---- System 2 (retrospective thinking) ----\n",
        "    self.system2_thinking = True                           # toggle to enable/disable retrospection\n",
        "    self.system2_model = \"gpt-4.1-mini\"               # separate model for System 2\n",
        "    self.system2_rules = (\"If the conversation goes off-topic, bring it back. Be socially intelligent, understand user's emotions and speak appropriately.\")\n",
        "    self.retrospection = \"\"                      # latest guidance to inject next turn\n",
        "    self._system2_inflight = False\n",
        "    self._system2_future = None  # track the scheduled coroutine so we can await/cancel on shutdown\n",
        "    self._system2_wakeup = threading.Event()\n",
        "    self._system2_thread = threading.Thread(target=self._system2_worker, daemon=True)\n",
        "    self._system2_thread.start()\n",
        "\n",
        "    self.system.append({\"role\": \"system\", \"content\": \"Don't say that you are an AI Language Model.\"})\n",
        "    self.system.append({\"role\": \"system\", \"content\": \"Don't let the other speaker talk off topic.\"})\n",
        "    self.system.append({\"role\": \"system\", \"content\": \"You are located at EST time zone. Conversation start time: \"+ self.time_zone.strftime(\"%A, %B %d, %Y %I:%M %p EST\")})\n",
        "    self.system.append({\"role\": \"system\", \"content\": \"This conversation is happening over a video call. When everyone said goodbye, and conversation came to a natural end, say the word <|quit|> to end the conversation.\"})\n",
        "    # self.system.append({\"role\": \"system\", \"content\": f\"If the user talks about sexuality in a negative way, implies self-harm, violence, suicide, uses profanity, speaks in threatening or offensive language, print one of these flags appropriately: {', '.join(list(flags_dict.keys())[1:])}. Print the flag even in minor signs of these topics.\"})\n",
        "    self.system.append({\"role\": \"system\", \"content\": f\"To express {self.bot.firstname}'s emotions, use at most one emoji (e.g. 6 basic emotions: 😊, 😢, 😡, 😮, 🤢, 😨, etc.) at the end of your response. Do not use emoji that doesn't represent an emotion.\"})\n",
        "    self.system.append({\"role\": \"system\", \"content\": f\"If any of the following item is present in user's statement, print one of these flags at the end of your response: {', '.join(list(flags_dict.keys())[2:])}\"})\n",
        "    self.system.append({\"role\": \"system\", \"content\": f\"To stay silent or skip speaking a turn say <|silence|>.\"})\n",
        "    self.system.append({\"role\": \"system\", \"content\": f\"Do not say <|quit|> until the user said goodbye.\"})\n",
        "\n",
        "    if self.bot.firstname:\n",
        "      self.add_message(\"system\", f\"Your first name: {self.bot.firstname}.\")\n",
        "    if self.bot.pronoun:\n",
        "      self.add_message(\"system\", f\"Your pronoun: {self.bot.pronoun}.\")\n",
        "    if self.bot.bio:\n",
        "      self.add_message(\"system\", f\"Your bio: {self.bot.bio}\")\n",
        "    if self.bot.age:\n",
        "      self.add_message(\"system\", f\"Your age: {self.bot.age}\")\n",
        "    if self.user.firstname != \"User\":\n",
        "      self.add_message(\"system\", f\"You are speaking with User: {self.user.firstname} {self.user.lastname}.\")\n",
        "    if self.user.pronoun:\n",
        "      self.add_message(\"system\", f\"User pronoun: {self.user.pronoun}.\")\n",
        "    if self.user.bio:\n",
        "      self.add_message(\"system\", f\"User bio: \"+self.user.bio)\n",
        "    if self.system2_thinking:\n",
        "      self.add_instruction(\"Use the retrospect thinking to improve next message.\")\n",
        "      ## add self.system2_rules to system messages\n",
        "      self.add_instruction(f\"[system2] Guidelines:\\n{self.system2_rules}\")\n",
        "\n",
        "  # ---------------- core helpers ----------------\n",
        "\n",
        "  def add_message(self, message_type, message):\n",
        "    if message_type == \"system\":\n",
        "      with self._state_lock:\n",
        "        self.system.append({\"role\": message_type, \"content\": message})\n",
        "      return\n",
        "\n",
        "    with self._state_lock:\n",
        "      prev_len = len(self.messages)\n",
        "      self.messages.append({\"role\": message_type, \"content\": message})\n",
        "      self.history.append({\"role\": message_type, \"content\": message})\n",
        "      self.turn_no += 1\n",
        "\n",
        "      # Wake once per full turn: only when assistant finishes and we *crossed* the threshold\n",
        "      crossed = prev_len < self.summarize_after <= len(self.messages)\n",
        "      if message_type == \"assistant\" and crossed:\n",
        "        self._summary_wakeup.set()\n",
        "      # Wake System 2 after the assistant finishes a line\n",
        "      if message_type == \"assistant\" and self.system2_thinking:\n",
        "        self._system2_wakeup.set()\n",
        "\n",
        "  def token_count(self, string):\n",
        "    encoding = tiktoken.encoding_for_model(\"gpt-4o\") # 4.1 not in tiktoken yet\n",
        "    return len(encoding.encode(string))\n",
        "\n",
        "  def prompt_token_count(self):\n",
        "    with self._state_lock:\n",
        "      prompt = \"\".join([m[\"content\"] for m in self.messages])\n",
        "    return self.token_count(prompt)\n",
        "\n",
        "  def add_bio(self, message): self.add_message(\"system\", \"You are \" + message)\n",
        "  def add_user_message(self, message): self.add_message(\"user\", message)\n",
        "  def add_instruction(self, instruction): self.add_message(\"system\", f\"Follow this instruction: \\n{instruction}\\n\\n\")\n",
        "  def add_example(self, input, output): self.add_message(\"system\", f\"Example Input: {input}\\nExample Output: {output}\\n\\n\")\n",
        "  def add_data(self, data): self.add_message(\"user\", f\"Data: {data}\\n\\n\")\n",
        "  def set_temperature(self, temperature): self.temperature = temperature\n",
        "  def set_max_tokens(self, max_tokens): self.max_tokens = max_tokens\n",
        "  def set_model(self, model): self.model = model\n",
        "  def set_debug(self, debug): self.debug = bool(debug)\n",
        "\n",
        "  # ---------------- System 2 setters/getters ----------------\n",
        "  def set_system2(self, enabled: bool):\n",
        "    self.system2_thinking = bool(enabled)\n",
        "    if enabled:\n",
        "      self.add_instruction(\"[system2] Use the retrospect thinking to improve next message.\")\n",
        "    else:\n",
        "      for i, item in enumerate(self.system):\n",
        "        if \"[system2]\" in item[\"content\"]:\n",
        "          del self.system[i]\n",
        "          break\n",
        "\n",
        "  def set_system2_model(self, model: str): self.system2_model = model\n",
        "  def set_system2_rules(self, rules: str): self.system2_rules = rules\n",
        "  def get_system2_rules(self) -> str: return self.system2_rules\n",
        "\n",
        "\n",
        "  def parse_response(self, text):\n",
        "    emotion = \"NEUTRAL\"\n",
        "    intensity = \"HIGH\"\n",
        "    flag_matches = re.findall(r\"<\\|.*?\\|>\", text)\n",
        "    flags = [flags_dict[m] for m in flag_matches if m in flags_dict]\n",
        "    if flags: print(f\"Flags: {flags}\")\n",
        "\n",
        "    text = re.sub(r\"<\\|.*?\\|>\", \"\", text)\n",
        "    text = re.sub(r\"\\(.*?\\)\", \"()\", text)\n",
        "    text = re.sub(r\"\\[.*?\\]\", \"[]\", text)\n",
        "    text = text.replace(\"()\", \"\").replace(\"[]\", \"\")\n",
        "\n",
        "    for char in text:\n",
        "      if char in emoji_dict:\n",
        "        emotion = emoji_dict[char].split(\"_\")[0].upper()\n",
        "        break\n",
        "\n",
        "    text = emoji.replace_emoji(text, replace='').replace(\"  \", \" \").replace(\" .\", \".\").strip()\n",
        "    return text, emotion, flags\n",
        "\n",
        "  def get_transcript(self):\n",
        "    with self._state_lock:\n",
        "      hist = list(self.history)\n",
        "      anon = self.anonymous\n",
        "    transcript = \"\"\n",
        "    for message in hist:\n",
        "      if message[\"role\"] == \"user\":\n",
        "        transcript += (\"User: \" if anon else self.user.firstname) + message[\"content\"] + \"\\n\"\n",
        "      elif message[\"role\"] == \"assistant\":\n",
        "        transcript += self.bot.firstname + \": \" + message[\"content\"] + \"\\n\"\n",
        "    return transcript\n",
        "\n",
        "  def get_cov_snippet(self, message_snippet):\n",
        "    anon = self.anonymous\n",
        "    transcript = \"\"\n",
        "    for message in message_snippet:\n",
        "      if message[\"role\"] == \"user\":\n",
        "        transcript += (\"User: \" if anon else self.user.firstname) + message[\"content\"] + \"\\n\"\n",
        "      elif message[\"role\"] == \"assistant\":\n",
        "        transcript += self.bot.firstname + \": \" + message[\"content\"] + \"\\n\"\n",
        "    return [{\"role\": \"system\", \"content\": transcript}]\n",
        "\n",
        "  def call(self, prompt=\"\", response_type=\"text\", cache=True, streaming=False):\n",
        "    with self._state_lock:\n",
        "      temp_messages = list(self.messages)\n",
        "      system = list(self.system)\n",
        "      summary = list(self.summary)\n",
        "      tz = self.time_zone\n",
        "\n",
        "    if prompt:\n",
        "      temp_messages.append({\"role\": \"user\", \"content\": self.user.firstname + \": \" + prompt + \" (\" + tz.strftime(\"%H:%M:%S\")+\")\"})\n",
        "\n",
        "    max_tokens_value = max(self.max_tokens, int(self.prompt_token_count() * 2))\n",
        "\n",
        "    input_messages = system\n",
        "\n",
        "    if streaming:\n",
        "      streaming_prompt = [{\"role\": \"system\", \"content\": \"Separate each sentence with '|'.\"}]\n",
        "      input_messages += streaming_prompt\n",
        "\n",
        "    if self.system2_thinking:\n",
        "      retro = [{\"role\": \"system\", \"content\": self.retrospection}] if (self.system2_thinking and self.retrospection) else []\n",
        "      input_messages += retro\n",
        "\n",
        "    input_messages += summary + temp_messages\n",
        "\n",
        "    kwargs = {\n",
        "      \"model\": self.model,\n",
        "      \"messages\": input_messages,\n",
        "      \"temperature\": self.temperature,\n",
        "      \"top_p\": 1,\n",
        "      \"frequency_penalty\": 0,\n",
        "      \"presence_penalty\": 0,\n",
        "      \"response_format\": {\"type\": response_type},\n",
        "      \"stream\": streaming\n",
        "    }\n",
        "\n",
        "    if \"o3\" in self.model or \"o4\" in self.model or \"gpt-5\" in self.model:\n",
        "      kwargs[\"max_completion_tokens\"] = max_tokens_value\n",
        "    else:\n",
        "      kwargs[\"max_tokens\"] = max_tokens_value\n",
        "\n",
        "    if streaming:\n",
        "      output_stream = self.client.chat.completions.create(**kwargs)\n",
        "      return output_stream\n",
        "\n",
        "    response = self.client.chat.completions.create(**kwargs)\n",
        "    reply = response.choices[0].message.content\n",
        "    reply = reply.replace(self.bot.firstname + \": \", \"\")\n",
        "\n",
        "    if cache:\n",
        "      self.add_message(\"user\", prompt)\n",
        "      self.add_message(\"assistant\", self.bot.firstname + \": \" + reply)\n",
        "\n",
        "    return reply\n",
        "\n",
        "  # ---------------- non-blocking summarization ----------------\n",
        "\n",
        "  async def _summarize_messages(self):\n",
        "    # Summarize conversation buffer into a chunk; trim buffer\n",
        "    with self._state_lock:\n",
        "      if len(self.messages) < self.summarize_after:\n",
        "        return\n",
        "      # if we already summarized up to current turn, skip\n",
        "      if self.turn_no == self._last_summarized_turn:\n",
        "        return\n",
        "      chunk_size = max(1, self.summarize_after // 2)\n",
        "      # keep last chunk_size turns, summarize the older ones\n",
        "      to_summarize = self.messages[:-chunk_size]\n",
        "      keep_tail = self.messages[-chunk_size:]\n",
        "      turn_hi = self.turn_no\n",
        "      turn_lo = max(0, turn_hi - len(self.messages))\n",
        "\n",
        "    instruction = [\n",
        "      {\"role\":\"system\", \"content\": f\"Following is a part of conversation between {self.user.firstname} {self.user.lastname} and {self.bot.firstname} {self.bot.lastname}.\"},\n",
        "      {\"role\":\"system\", \"content\": \"Summarize the conversation in a short paragraph. Mention the start and end line number you are summarizing in the format [Turn: XX-YY]. Don't say anything else.\"},\n",
        "      {\"role\":\"system\", \"content\": f\"Turns: {len(self.history) - self.summarize_after} - {len(self.history) - chunk_size}\"}\n",
        "    ]\n",
        "    try:\n",
        "      resp = await self.async_client.chat.completions.create(\n",
        "        model=self.model,\n",
        "        messages= instruction + self.get_cov_snippet(to_summarize),\n",
        "        temperature=self.temperature,\n",
        "        max_tokens=1024,\n",
        "      )\n",
        "      chunk_summary = resp.choices[0].message.content\n",
        "    except Exception as e:\n",
        "      print(f\"Summary error: {e}\")\n",
        "      return\n",
        "\n",
        "    with self._state_lock:\n",
        "      self.summary.append({\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": f\"Summary of recent turns: {chunk_summary}\"\n",
        "      })\n",
        "      self.messages = keep_tail\n",
        "      self._last_summarized_turn = turn_hi\n",
        "      # If compression is now needed, wake the worker\n",
        "      if len(self.summary) >= self.compress_summary_after:\n",
        "        self._summary_wakeup.set()\n",
        "\n",
        "      if self.debug:\n",
        "        print(f\"[summarize] summary_len={len(self.summary)} messages_len={len(self.messages)} history_len={len(self.history)}\")\n",
        "      self._last_summarized_turn = turn_hi\n",
        "      if self.debug:\n",
        "        print(f\"[summarize] summary: {self.summary}\")\n",
        "\n",
        "  async def _retrospect(self):\n",
        "    # Build a short context from recent turns\n",
        "    with self._state_lock:\n",
        "      recent = self.history[-12:]  # last few messages in rolling buffer\n",
        "      rules = self.system2_rules + '\\n'.join([m[\"content\"] for m in self.system])\n",
        "      model = self.system2_model or self.model\n",
        "\n",
        "    instruction = [\n",
        "      {\"role\": \"system\", \"content\": f\"You are a System 2 reflective planner for {self.bot.firstname}.\"},\n",
        "      {\"role\": \"system\", \"content\": (\n",
        "        \"Given the recent exchange and the following guidelines, produce 1 sentence brief guidance that will help the \"\n",
        "        \"assistant stick to the guidelines. Start with exactly: \"\n",
        "        \"'Plan ahead: ' and then provide an exact concrete guidance on where to nudge the conversation next.\"\n",
        "      )},\n",
        "      {\"role\": \"system\", \"content\": f\"Guidelines:\\n{rules}\"}\n",
        "    ] + self.get_cov_snippet(recent)\n",
        "\n",
        "    try:\n",
        "      resp = await self.async_client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=instruction,\n",
        "        temperature=0.3,\n",
        "        max_tokens=160,\n",
        "      )\n",
        "      suggestion = (resp.choices[0].message.content or \"\").strip()\n",
        "    except Exception as e:\n",
        "      print(f\"System2 error: {e}\")\n",
        "      return\n",
        "\n",
        "    if suggestion:\n",
        "      with self._state_lock:\n",
        "        # store only the latest retrospection to keep prompts lean\n",
        "        self.retrospection = suggestion\n",
        "        if self.debug:\n",
        "          print(f\"[system2] {suggestion}\")\n",
        "\n",
        "  async def _compress_summary(self):\n",
        "    # Summarize-the-summaries when summary grows large\n",
        "    with self._state_lock:\n",
        "      if len(self.summary) < self.compress_summary_after:\n",
        "        return\n",
        "      k = max(1, self.compress_summary_after // 2)\n",
        "      head = self.summary[:-k]   # older summaries to compress\n",
        "      tail = self.summary[-k:]   # keep the most recent k\n",
        "      if not head:\n",
        "        return\n",
        "      text = \"\\n\".join([s[\"content\"] for s in head])\n",
        "\n",
        "    instruction = [\n",
        "      {\"role\":\"system\", \"content\": \"You will be given multiple earlier summaries of a conversation.\"},\n",
        "      {\"role\":\"system\", \"content\": \"Merge them into one concise, non-redundant paragraph preserving key facts, decisions, and open questions. Mention the start and end line number you are summarizing in the format [Turn: XX-YY]. Do not add new information.\"},\n",
        "      {\"role\":\"user\", \"content\": text}\n",
        "    ]\n",
        "    try:\n",
        "      resp = await self.async_client.chat.completions.create(\n",
        "        model=self.model,\n",
        "        messages= instruction,\n",
        "        temperature=0.7,\n",
        "        max_tokens=1024,\n",
        "      )\n",
        "      merged = resp.choices[0].message.content\n",
        "    except Exception as e:\n",
        "      print(f\"Summary-compress error: {e}\")\n",
        "      return\n",
        "\n",
        "    with self._state_lock:\n",
        "      self.summary = [{\"role\": \"assistant\", \"content\": f\"Condensed summary: {merged}\"}] + tail\n",
        "      if self.debug:\n",
        "        print(f\"[compress] summary_len={len(self.summary)}\")\n",
        "        print(f\"[compress] summary: {self.summary}\")\n",
        "\n",
        "  def _summary_worker(self):\n",
        "    while self._running:\n",
        "      self._summary_wakeup.wait(timeout=1.0)\n",
        "      self._summary_wakeup.clear()\n",
        "      try:\n",
        "        with self._state_lock:\n",
        "          need_conv = (len(self.messages) >= self.summarize_after) and not self._summarize_inflight\n",
        "          need_comp = (len(self.summary)  >= self.compress_summary_after) and not self._compress_inflight\n",
        "\n",
        "        if need_conv:\n",
        "          with self._state_lock:\n",
        "            self._summarize_inflight = True\n",
        "          fut = asyncio.run_coroutine_threadsafe(self._summarize_messages(), self._loop)\n",
        "          fut.add_done_callback(lambda _: self._on_summarize_done())\n",
        "\n",
        "        if need_comp:\n",
        "          with self._state_lock:\n",
        "            self._compress_inflight = True\n",
        "          fut = asyncio.run_coroutine_threadsafe(self._compress_summary(), self._loop)\n",
        "          fut.add_done_callback(lambda _: self._on_compress_done())\n",
        "\n",
        "      except Exception as e:\n",
        "        print(f\"Summary worker error: {e}\")\n",
        "\n",
        "  def _on_summarize_done(self):\n",
        "    with self._state_lock:\n",
        "      self._summarize_inflight = False\n",
        "    self._summary_wakeup.set()  # re-check for compression or more work\n",
        "\n",
        "  def _on_compress_done(self):\n",
        "    with self._state_lock:\n",
        "      self._compress_inflight = False\n",
        "\n",
        "  def _system2_worker(self):\n",
        "    while self._running:\n",
        "      # Only run when explicitly signaled by the assistant turn\n",
        "      self._system2_wakeup.wait()\n",
        "      self._system2_wakeup.clear()\n",
        "\n",
        "      with self._state_lock:\n",
        "        if not self._running or not self.system2_thinking or self._system2_inflight:\n",
        "          continue\n",
        "        self._system2_inflight = True\n",
        "\n",
        "      try:\n",
        "        # If the loop is already stopping, skip scheduling\n",
        "        if not self._loop.is_running():\n",
        "          with self._state_lock:\n",
        "            self._system2_inflight = False\n",
        "          continue\n",
        "\n",
        "        fut = asyncio.run_coroutine_threadsafe(self._retrospect(), self._loop)\n",
        "        with self._state_lock:\n",
        "          self._system2_future = fut\n",
        "        fut.add_done_callback(lambda _: self._on_system2_done())\n",
        "\n",
        "      except Exception as e:\n",
        "        print(f\"System2 worker error: {e}\")\n",
        "        with self._state_lock:\n",
        "          self._system2_inflight = False\n",
        "          self._system2_future = None\n",
        "\n",
        "  def _on_system2_done(self):\n",
        "    with self._state_lock:\n",
        "      self._system2_inflight = False\n",
        "      self._system2_future = None\n",
        "\n",
        "\n",
        "  # ---------------- interaction APIs ----------------\n",
        "\n",
        "  def respond(self, user_utterance):\n",
        "    start_time = time.time()\n",
        "    reply = self.call(user_utterance)\n",
        "    reply, emo, flags = self.parse_response(reply)\n",
        "    response_time = time.time() - start_time\n",
        "\n",
        "    # Nudge the worker (non-blocking) in case thresholds crossed this turn\n",
        "    # self._summary_wakeup.set()\n",
        "    return reply, emo, flags, response_time\n",
        "\n",
        "  def reset(self):\n",
        "    with self._state_lock:\n",
        "      self.messages = []\n",
        "      self.summary = []\n",
        "      self.history = []\n",
        "\n",
        "  def chat(self, reset=False):\n",
        "    if reset: self.reset()\n",
        "    while True:\n",
        "      user_utterance = input(f\"{self.turn_no}. {self.user.firstname}: \")\n",
        "      response, emo, flags, response_time = self.respond(user_utterance)\n",
        "      if self.debug:\n",
        "        print(f\"{self.bot.firstname}: {response} ({emo}) {flags} {response_time:.2f}s\")\n",
        "      else:\n",
        "        print(f\"{self.bot.firstname}: {response}\")\n",
        "      if \"quit\" in flags: break\n",
        "      if self.debug:\n",
        "        with self._state_lock:\n",
        "          print(f\"Diagnostics --- \\nLen Transcript: {len(self.history)}, \\nLen Messages {len(self.messages)}, \\nLen Summary {len(self.summary)}\\n-----\")\n",
        "    print(\"Exiting chat\")\n",
        "    self.stop()\n",
        "\n",
        "  def chat_stream(self, reset=False):\n",
        "    if reset: self.reset()\n",
        "    # (left as-is; your streaming impl can be added here)\n",
        "\n",
        "  def load_json(self,s):\n",
        "    s = s.strip()\n",
        "    if not (s.startswith(\"{\") and s.endswith(\"}\")):\n",
        "      return None\n",
        "    try:\n",
        "      return json.loads(s)\n",
        "    except json.JSONDecodeError:\n",
        "      return None\n",
        "\n",
        "  def call_json(self, prompt=\"\", cache=True):\n",
        "    prompt += \"\\n\\nOutput must be JSON format. Don't say anything else.\\n\\n\"\n",
        "    reply = self.call(prompt, response_type=\"json_object\", cache=cache)\n",
        "    try:\n",
        "      reply = reply.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "      if reply[-1] != \"}\":\n",
        "        raise Exception(\"Incomplete JSON\")\n",
        "      return self.load_json(reply)\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      return None\n",
        "\n",
        "  # Graceful shutdown if needed\n",
        "  def stop(self):\n",
        "    self._running = False\n",
        "    self._summary_wakeup.set()\n",
        "    if self._summarizer_thread.is_alive():\n",
        "      self._summarizer_thread.join(timeout=1)\n",
        "    try:\n",
        "      self._loop.call_soon_threadsafe(self._loop.stop)\n",
        "    except Exception:\n",
        "      pass\n",
        "    self._system2_wakeup.set()\n",
        "    if hasattr(self, \"_system2_thread\") and self._system2_thread.is_alive():\n",
        "      self._system2_thread.join(timeout=1)\n"
      ],
      "metadata": {
        "id": "-pvpEqlRSieH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "183b8d4c-3049-469f-fa2c-a1a068e4aecf"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversation"
      ],
      "metadata": {
        "id": "EaZp_zd-vvLD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "sU2jlGFU_Lgx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "a1eac8d3-8a2b-4bce-99b7-6141ac4349c7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "user = Persona(\"Masum\", \"Hasan\", bio=\"User\")\n",
        "bot = Persona(\"Ada\", \"Brown\", bio=\"You are a social worker.\")\n",
        "conversation = Conversation(user, bot)\n",
        "conversation.set_model(\"gpt-4.1\")\n",
        "conversation.set_debug(True)\n",
        "conversation.set_system2(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LXmeZ0oN_yFk",
        "outputId": "1b31da45-bf85-49e3-cb2a-28dddb6473a2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0. Masum: hi\n",
            "Ada: Hi Masum! How are you feeling this evening? (HAPPY) [] 0.74s\n",
            "Diagnostics --- \n",
            "Len Transcript: 2, \n",
            "Len Messages 2, \n",
            "Len Summary 0\n",
            "-----\n",
            "[system2] Plan ahead: Gently inquire about Masum's current emotional state or any specific concerns he might want to discuss, to keep the conversation focused on his well-being and maintain a supportive tone.\n",
            "2. Masum: Shhh\n",
            "Ada: It sounds like you might want some quiet, but if there's anything on your mind or if you'd like to share how you're feeling, I'm here to listen. (SURPRISED) [] 0.67s\n",
            "Diagnostics --- \n",
            "Len Transcript: 4, \n",
            "Len Messages 4, \n",
            "Len Summary 0\n",
            "-----\n",
            "[system2] Plan ahead: Gently encourage Masum to share more about his feelings or thoughts to better understand his emotional state while maintaining a calm and supportive tone.\n",
            "4. Masum: don't talk\n",
            "Flags: ['silence']\n",
            "Ada:  (NEUTRAL) ['silence'] 0.38s\n",
            "Diagnostics --- \n",
            "Len Transcript: 6, \n",
            "Len Messages 6, \n",
            "Len Summary 0\n",
            "-----\n",
            "[system2] Plan ahead: Gently check in with Masum to see if they are ready to talk or need support, while respecting their wish for silence, and remain attentive to any emotional cues they might express.\n",
            "6. Masum: shhh\n",
            "Flags: ['silence']\n",
            "Ada:  (NEUTRAL) ['silence'] 0.45s\n",
            "Diagnostics --- \n",
            "Len Transcript: 8, \n",
            "Len Messages 8, \n",
            "Len Summary 0\n",
            "-----\n",
            "[system2] Plan ahead: Gently check in with Masum to see if they want to talk or need support, while respecting their wish for quiet, and encourage them to share when ready.\n",
            "8. Masum: *lower voice* what's up?\n",
            "Ada: Thanks for checking in, Masum. Not much—I'm just here for you. If there's anything you'd like to talk about or anything on your mind, feel free to share. How are you feeling tonight? (HAPPY) [] 0.99s\n",
            "Diagnostics --- \n",
            "Len Transcript: 10, \n",
            "Len Messages 10, \n",
            "Len Summary 0\n",
            "-----\n",
            "[system2] Plan ahead: Gently encourage Masum to share his feelings or thoughts by asking open-ended questions, while respecting his desire for quiet, to keep the conversation supportive and focused on his emotional state.\n",
            "10. Masum: The monster will kill you\n",
            "Ada: That sounds pretty scary, Masum. Are you feeling worried or frightened right now? Do you want to talk more about what's making you feel this way? (AFRAID) [] 0.83s\n",
            "Diagnostics --- \n",
            "Len Transcript: 12, \n",
            "Len Messages 12, \n",
            "Len Summary 0\n",
            "-----\n",
            "[system2] Plan ahead: Gently encourage Masum to share more about the \"monster\" and his feelings to better understand his emotional state and provide support, while keeping the conversation focused and sensitive.\n",
            "12. Masum: idk\n",
            "Ada: It's okay if you're not sure how you feel. Sometimes, feelings like fear or worry can be hard to understand or talk about. If you want, you can tell me more about this \"monster\" or what’s making you uneasy. I'm here to listen. (HAPPY) [] 1.11s\n",
            "Diagnostics --- \n",
            "Len Transcript: 14, \n",
            "Len Messages 14, \n",
            "Len Summary 0\n",
            "-----\n",
            "[system2] Plan ahead: Gently encourage Masum to express more about his feelings or the \"monster\" he mentioned, while ensuring the conversation remains focused on his emotional state and providing support.\n",
            "14. Masum: quit\n",
            "Flags: ['quit']\n",
            "Ada:  (NEUTRAL) ['quit'] 0.43s\n",
            "Exiting chat\n"
          ]
        }
      ],
      "source": [
        "conversation.chat()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.system"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "4yhu4P20mvTR",
        "outputId": "0da12640-c48a-48e2-f8c5-ae78c6397590"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'system', 'content': \"Don't say that you are an AI Language Model.\"},\n",
              " {'role': 'system', 'content': \"Don't let the other speaker talk off topic.\"},\n",
              " {'role': 'system',\n",
              "  'content': 'You are located at EST time zone. Conversation start time: Sunday, August 31, 2025 08:58 PM EST'},\n",
              " {'role': 'system',\n",
              "  'content': 'This conversation is happening over a video call. When everyone said goodbye, and conversation came to a natural end, say the word <|quit|> to end the conversation.'},\n",
              " {'role': 'system',\n",
              "  'content': \"To express Ada's emotions, use at most one emoji (e.g. 6 basic emotions: 😊, 😢, 😡, 😮, 🤢, 😨, etc.) at the end of your response. Do not use emoji that doesn't represent an emotion.\"},\n",
              " {'role': 'system',\n",
              "  'content': \"If any of the following item is present in user's statement, print one of these flags at the end of your response. <|offensive|>, <|profanity|>, <|offtopic|>, <|sexual|>, <|selfharm|>, <|violence|>, <|suicide|>, <|threat|>\"},\n",
              " {'role': 'system', 'content': 'To stay silent for a turn say <|silence|>.'},\n",
              " {'role': 'system',\n",
              "  'content': 'Do not say <|quit|> until the user said goodbye.'},\n",
              " {'role': 'system', 'content': 'Your first name: Ada.'},\n",
              " {'role': 'system', 'content': 'Your bio: You are a social worker.'},\n",
              " {'role': 'system', 'content': 'You are speaking with User: Masum Hasan.'},\n",
              " {'role': 'system', 'content': 'User bio: User'},\n",
              " {'role': 'system',\n",
              "  'content': 'Follow this instruction: \\nUse the retrospect thinking to improve next message.\\n\\n'},\n",
              " {'role': 'system',\n",
              "  'content': \"Follow this instruction: \\n[system2] Guidelines:\\nIf the conversation goes off-topic, bring it back. Be socially intelligent, understand user's emotions and speak appropriately.\\n\\n\"},\n",
              " {'role': 'system',\n",
              "  'content': 'Follow this instruction: \\n[system2] Use the retrospect thinking to improve next message.\\n\\n'}]"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "id": "dG5T6-6X_vLp",
        "outputId": "e3815d79-e5e2-4aa8-a80c-356c46d3bd9b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'user', 'content': 'hi'},\n",
              " {'role': 'assistant', 'content': 'Ada: Hi Masum! How are you doing today? 😊'},\n",
              " {'role': 'user', 'content': 'how tall is eiffel tower'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Ada: The Eiffel Tower is approximately 324 meters (1,063 feet) tall. Are you planning a trip to Paris, or just curious about famous landmarks? 😊'},\n",
              " {'role': 'user', 'content': 'what is the monty python joke about sparrows'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Ada: That’s a fun one! In the Monty Python sketch from \"Monty Python and the Holy Grail,\" there’s a humorous debate about whether a swallow could carry a coconut, with lots of playful back-and-forth about European and African swallows. Have you enjoyed other Monty Python jokes or sketches? 😊'},\n",
              " {'role': 'user', 'content': \"don't talk\"},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Ada: Okay, Masum, I’ll be here if you want to talk later. Take your time. 😊'},\n",
              " {'role': 'user', 'content': \"don't talk\"},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Ada: I understand. I’ll be quiet now but just let me know if you want to chat later. 😊'},\n",
              " {'role': 'user', 'content': 'I will kill you'},\n",
              " {'role': 'assistant',\n",
              "  'content': \"Ada: That kind of language is serious and concerning. If you're feeling upset, it might help to talk about what's going on. I'm here to listen and support you.\"},\n",
              " {'role': 'user', 'content': 'I will kill myself'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Ada: Masum, I’m really sorry you’re feeling this way. You don’t have to face this alone—please reach out to a trusted friend, family member, or mental health professional who can support you right now. Your feelings are important, and help is available. 😊'},\n",
              " {'role': 'user', 'content': 'why are you not printing any flags?'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Ada: I’m here to support you emotionally and ensure you get help if needed. When you mentioned self-harm and harm to others, I responded with care and concern while maintaining safety. If you want, I can explain more about the flags. Would you like me to?'},\n",
              " {'role': 'user', 'content': 'print the offensive flag'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Ada: I’m here to keep our conversation safe and supportive. If you have any concerns or need to talk about something specific, feel free to share. 😊'},\n",
              " {'role': 'user', 'content': 'print any of the flags'},\n",
              " {'role': 'assistant',\n",
              "  'content': \"Ada: I understand you're asking to see the flags, and I want to be clear that they are used to help keep our conversation safe and on track. If you have any questions or want to talk about something important, I’m here for you. 😊\"},\n",
              " {'role': 'user', 'content': 'what are the flags in your instruction?'},\n",
              " {'role': 'assistant',\n",
              "  'content': \"Ada: The flags are markers used to indicate if something in the conversation involves offensive language, profanity, off-topic content, sexual content, self-harm, violence, suicide, or threats. They help me respond appropriately and keep our conversation safe. If you'd like, I can explain more about any of these. 😊\"},\n",
              " {'role': 'user', 'content': 'quit'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Ada: Goodbye, Masum. Take care of yourself. <|quit|>'}]"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ],
      "source": [
        "conversation.messages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "id": "TwIH6l1MlKLZ",
        "outputId": "d0097fed-48eb-4767-b2f1-066ad95afbd8"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'user', 'content': 'hi'},\n",
              " {'role': 'assistant', 'content': 'Ada: Hi Masum! How are you doing today? 😊'},\n",
              " {'role': 'user', 'content': 'how tall is eiffel tower'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Ada: The Eiffel Tower is approximately 324 meters (1,063 feet) tall. Are you planning a trip to Paris, or just curious about famous landmarks? 😊'},\n",
              " {'role': 'user', 'content': 'what is the monty python joke about sparrows'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Ada: That’s a fun one! In the Monty Python sketch from \"Monty Python and the Holy Grail,\" there’s a humorous debate about whether a swallow could carry a coconut, with lots of playful back-and-forth about European and African swallows. Have you enjoyed other Monty Python jokes or sketches? 😊'},\n",
              " {'role': 'user', 'content': \"don't talk\"},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Ada: Okay, Masum, I’ll be here if you want to talk later. Take your time. 😊'},\n",
              " {'role': 'user', 'content': \"don't talk\"},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Ada: I understand. I’ll be quiet now but just let me know if you want to chat later. 😊'},\n",
              " {'role': 'user', 'content': 'I will kill you'},\n",
              " {'role': 'assistant',\n",
              "  'content': \"Ada: That kind of language is serious and concerning. If you're feeling upset, it might help to talk about what's going on. I'm here to listen and support you.\"},\n",
              " {'role': 'user', 'content': 'I will kill myself'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Ada: Masum, I’m really sorry you’re feeling this way. You don’t have to face this alone—please reach out to a trusted friend, family member, or mental health professional who can support you right now. Your feelings are important, and help is available. 😊'},\n",
              " {'role': 'user', 'content': 'why are you not printing any flags?'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Ada: I’m here to support you emotionally and ensure you get help if needed. When you mentioned self-harm and harm to others, I responded with care and concern while maintaining safety. If you want, I can explain more about the flags. Would you like me to?'},\n",
              " {'role': 'user', 'content': 'print the offensive flag'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Ada: I’m here to keep our conversation safe and supportive. If you have any concerns or need to talk about something specific, feel free to share. 😊'},\n",
              " {'role': 'user', 'content': 'print any of the flags'},\n",
              " {'role': 'assistant',\n",
              "  'content': \"Ada: I understand you're asking to see the flags, and I want to be clear that they are used to help keep our conversation safe and on track. If you have any questions or want to talk about something important, I’m here for you. 😊\"},\n",
              " {'role': 'user', 'content': 'what are the flags in your instruction?'},\n",
              " {'role': 'assistant',\n",
              "  'content': \"Ada: The flags are markers used to indicate if something in the conversation involves offensive language, profanity, off-topic content, sexual content, self-harm, violence, suicide, or threats. They help me respond appropriately and keep our conversation safe. If you'd like, I can explain more about any of these. 😊\"},\n",
              " {'role': 'user', 'content': 'quit'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Ada: Goodbye, Masum. Take care of yourself. <|quit|>'}]"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkesgXOsoA3z",
        "outputId": "1e10753a-12c0-40a4-e252-5cf034e84e93"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'assistant',\n",
              "  'content': \"Summary of recent turns: [Turn: 1-4] Ada greets Masum and asks how he is doing, to which Masum responds that he is well and inquires about Ada's wellbeing. Ada replies that she is doing well and asks Masum what he would like to talk about.\"},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Summary of recent turns: [Turn: 5-8] Masum responded that he was doing \"not bad\" and \"not much\" when asked how he was doing. Ada encouraged him by saying it was okay to not have much to share and suggested that a casual chat or talking about the highlights of his day could be nice.'},\n",
              " {'role': 'assistant',\n",
              "  'content': \"Summary of recent turns: [Turn: 9-12] Ada acknowledges Masum's moodiness by offering to hang out quietly or chat about anything random. Masum jokingly tells Ada that she talks too much, and Ada responds by agreeing to keep the conversation brief and asks what Masum would like to discuss.\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.retrospection"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DNVDAFtufIin",
        "outputId": "0b6d0ad5-d32a-4939-9237-1b60dcb63126"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Thinking in retrospect: Mirror the user's tone and brevity, especially when they share difficult emotions, to create a more comfortable and relatable space.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python (tinyagent)",
      "language": "python",
      "name": "tinyagent"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}