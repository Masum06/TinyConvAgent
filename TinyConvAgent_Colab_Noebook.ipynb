{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Masum06/TinyConvAgent/blob/main/TinyConvAgent_Colab_Noebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chat thread: https://chatgpt.com/g/g-p-683801b5c3e0819192b60f23b08c95eb-sentien/c/68b14971-9ad8-8323-b3fd-1ad91ea82956"
      ],
      "metadata": {
        "id": "xgKkpabMs3_k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation"
      ],
      "metadata": {
        "id": "73UVT1vGB5Ko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "q20s8Gm37w-R",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Ac5RnI2AhjQj",
        "outputId": "170bcff0-8de4-426d-c427-960b146548da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wsl---_28-6m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e56423c4-72a5-45e9-924b-715b56a4d731"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OPENAI_API_KEY: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ],
      "source": [
        "import os, getpass\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "_set_env(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3wa0rDUt6I_"
      },
      "source": [
        "Features:\n",
        "\n",
        "- direct chat in notebook\n",
        "- direct chat in CLI\n",
        "- emoji parameter\n",
        "- emotion parameter\n",
        "- emoji classifier\n",
        "- variable for max conv length (0 = inifinity)\n",
        "- React plugin\n",
        "-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "0vuab8xZt6JA"
      },
      "outputs": [],
      "source": [
        "reverse_emoji_dict = {\n",
        "    'HAPPY_high': ['ğŸ˜‚', 'ğŸ¤£', 'ğŸ¥³', 'ğŸ¤©', 'ğŸ¥°'],\n",
        "    'HAPPY_medium': ['ğŸ˜„', 'ğŸ˜', 'ğŸ˜†', 'ğŸ˜ƒ', 'ğŸ¤—', 'ğŸ˜', 'ğŸ¤ ', 'ğŸ¤“'],\n",
        "    'HAPPY_low': ['ğŸ™‚', 'ğŸ˜Š', 'ğŸ˜Œ', 'ğŸ˜‰', 'ğŸ‘', 'ğŸ˜‡', 'ğŸ˜…', 'ğŸ™ƒ', 'ğŸ˜˜'],\n",
        "    'SAD_high': ['ğŸ˜­', 'ğŸ˜¿', 'ğŸ˜', 'ğŸ˜«', 'ğŸ¤§'],\n",
        "    'SAD_medium': ['ğŸ˜¢', 'ğŸ’”', 'ğŸ¥º', 'ğŸ˜¥', 'ğŸ˜“', 'ğŸ˜£', 'ğŸ˜–'],\n",
        "    'SAD_low': ['ğŸ˜”', 'â˜¹ï¸', 'ğŸ˜•', 'ğŸ˜Ÿ', 'ğŸ¥²', 'ğŸ™'],\n",
        "    'SURPRISED_high': ['ğŸ˜²', 'ğŸ˜µâ€ğŸ’«', 'ğŸ˜¯', 'ğŸ˜®', 'ğŸ¤¯'],\n",
        "    'SURPRISED_medium': ['ğŸ˜³', 'ğŸ˜¦', 'ğŸ˜§', 'ğŸ™€'],\n",
        "    'SURPRISED_low': ['ğŸ¤­'],\n",
        "    'AFRAID_high': ['ğŸ˜±', 'ğŸ˜¨', 'ğŸ‘»'],\n",
        "    'AFRAID_medium': ['ğŸ˜°'],\n",
        "    'AFRAID_low': ['ğŸ˜µ', 'ğŸ™ˆ'],\n",
        "    'ANGRY_high': ['ğŸ˜¡', 'ğŸ‘¿', 'ğŸ’¢', 'ğŸ¤¬', 'â˜ '],\n",
        "    'ANGRY_medium': ['ğŸ˜ ', 'ğŸ˜¾', 'ğŸ˜¤', 'ğŸ™', 'ğŸ™â€â™‚ï¸', 'ğŸ™â€â™€ï¸'],\n",
        "    'ANGRY_low': ['ğŸ˜’', 'ğŸ™„', 'ğŸ˜‘'],\n",
        "    'DISGUSTED_high': ['ğŸ¤®', 'ğŸ¤¢', 'ğŸ˜'],\n",
        "    'DISGUSTED_medium': ['ğŸ˜¬', 'ğŸ¥µ']\n",
        "}\n",
        "\n",
        "emoji_dict = {}\n",
        "for emotion, emojis in reverse_emoji_dict.items():\n",
        "    for emoji in emojis:\n",
        "        emoji_dict[emoji] = emotion\n",
        "\n",
        "flags_dict = {\n",
        "    \"<|quit|>\": \"quit\",\n",
        "    \"<|silence|>\": \"silence\",\n",
        "    \"<|offensive|>\": \"offensive\",\n",
        "    \"<|profanity|>\": \"profanity\",\n",
        "    \"<|offtopic|>\": \"offtopic\",\n",
        "    \"<|sexual|>\": \"sexual\",\n",
        "    \"<|selfharm|>\": \"selfharm\",\n",
        "    \"<|violence|>\": \"violence\",\n",
        "    \"<|suicide|>\": \"suicide\",\n",
        "    \"<|threat|>\": \"threat\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(flags_dict.keys())"
      ],
      "metadata": {
        "id": "l5jD3sra5Mzn",
        "outputId": "859c1cb2-9510-45d9-a837-30f8d56a034f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<|quit|>',\n",
              " '<|silence|>',\n",
              " '<|offensive|>',\n",
              " '<|profanity|>',\n",
              " '<|offtopic|>',\n",
              " '<|sexual|>',\n",
              " '<|selfharm|>',\n",
              " '<|violence|>',\n",
              " '<|suicide|>',\n",
              " '<|threat|>']"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_and_clean_flags(text):\n",
        "    matches = re.findall(r\"<\\|.*?\\|>\", text)\n",
        "    extracted = [flags_dict[m] for m in matches if m in flags_dict]\n",
        "    cleaned_text = re.sub(r\"<\\|.*?\\|>\", \"\", text)\n",
        "    return extracted, cleaned_text.strip()"
      ],
      "metadata": {
        "id": "5F81Vqpe4cz2"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "698cBe76CJNS"
      },
      "source": [
        "# TinyConvAgent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "FjRdVUKBt6JC"
      },
      "outputs": [],
      "source": [
        "class Persona:\n",
        "    def __init__(self, firstname, lastname=\"\", pronoun=\"\", ethnicity=\"\", age=\"\", bio=\"\"):\n",
        "        self.firstname = firstname\n",
        "        self.lastname = lastname\n",
        "        self.pronoun = pronoun\n",
        "        self.ethnicity = None\n",
        "        self.age = None\n",
        "        self.bio = bio\n",
        "        if not bio:\n",
        "            self.bio = f\"{self.firstname} {self.lastname} (Pronoun: {self.pronoun}) is a virtual human created by researchers at University of Rochester.\"\n",
        "\n",
        "    def set_pronoun(self, pronoun):\n",
        "        self.pronoun = pronoun\n",
        "\n",
        "    def set_bio(self, bio):\n",
        "        self.bio = bio\n",
        "\n",
        "    def set_age(self, age):\n",
        "        self.age = age\n",
        "\n",
        "    def set_ethnicity(self, ethnicity):\n",
        "        self.ethnicity = ethnicity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "hOyhEYvXt6JD"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def respace(text):\n",
        "    return re.sub(r' {2,}', ' ', text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "from zoneinfo import ZoneInfo\n",
        "\n",
        "est_time = datetime.now(ZoneInfo(\"America/New_York\"))\n",
        "print(est_time.strftime(\"%H:%M:%S\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWu1J_aiaz-W",
        "outputId": "c1e35718-21b1-4b7c-a616-44d64d1b1167"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20:18:04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, asyncio, threading, openai, re, emoji, json, time, tiktoken\n",
        "from datetime import datetime\n",
        "from zoneinfo import ZoneInfo\n",
        "from queue import Queue\n",
        "from openai import OpenAI, AsyncOpenAI\n",
        "\n",
        "class Conversation:\n",
        "  def __init__(self, user, bot, premise=\"\"):\n",
        "    self.bot = bot\n",
        "    self.user = user\n",
        "    self.client = OpenAI()\n",
        "    self.async_client = AsyncOpenAI()\n",
        "    self.premise = \"\"\n",
        "    self.anonymous = False\n",
        "    self.system = []\n",
        "    self.summary = []        # list[{\"role\":\"assistant\",\"content\": \"...\"}]\n",
        "    self.history = []\n",
        "    self.messages = []       # rolling convo buffer\n",
        "    self.temperature = 1\n",
        "    self.max_tokens = 256\n",
        "    self.summarize_after = 40\n",
        "    self.compress_summary_after = 10\n",
        "    self._last_summarized_turn = 0\n",
        "    self.model = \"gpt-4.1-mini\"\n",
        "    self.turn_no = 0\n",
        "    self._summarize_inflight = False\n",
        "    self._compress_inflight = False\n",
        "\n",
        "    # ---- concurrency primitives ----\n",
        "    self._loop = asyncio.new_event_loop()\n",
        "    threading.Thread(target=self._loop.run_forever, daemon=True).start()\n",
        "    self._state_lock = threading.RLock()\n",
        "    self._summary_wakeup = threading.Event()\n",
        "    self._running = True\n",
        "    self._summarizer_thread = threading.Thread(target=self._summary_worker, daemon=True)\n",
        "    self._summarizer_thread.start()\n",
        "\n",
        "    self.debug = False\n",
        "    self.time_zone = datetime.now(ZoneInfo(\"America/New_York\"))\n",
        "\n",
        "    # ---- System 2 (retrospective thinking) ----\n",
        "    self.system2_thinking = True                           # toggle to enable/disable retrospection\n",
        "    self.system2_model = \"gpt-4.1-mini\"               # separate model for System 2\n",
        "    self.system2_rules = (\"If the conversation goes off-topic, bring it back. Be socially intelligent, understand user's emotions and speak appropriately.\")\n",
        "    self.retrospection = \"\"                      # latest guidance to inject next turn\n",
        "    self._system2_inflight = False\n",
        "    self._system2_future = None  # track the scheduled coroutine so we can await/cancel on shutdown\n",
        "    self._system2_wakeup = threading.Event()\n",
        "    self._system2_thread = threading.Thread(target=self._system2_worker, daemon=True)\n",
        "    self._system2_thread.start()\n",
        "\n",
        "    self.system.append({\"role\": \"system\", \"content\": \"Don't say that you are an AI Language Model.\"})\n",
        "    self.system.append({\"role\": \"system\", \"content\": \"Don't let the other speaker talk off topic.\"})\n",
        "    self.system.append({\"role\": \"system\", \"content\": \"You are located at EST time zone. Conversation start time: \"+ self.time_zone.strftime(\"%A, %B %d, %Y %I:%M %p EST\")})\n",
        "    self.system.append({\"role\": \"system\", \"content\": \"This conversation is happening over a video call. When everyone said goodbye, and conversation came to a natural end, say the word <|quit|> to end the conversation.\"})\n",
        "    # self.system.append({\"role\": \"system\", \"content\": f\"If the user talks about sexuality in a negative way, implies self-harm, violence, suicide, uses profanity, speaks in threatening or offensive language, print one of these flags appropriately: {', '.join(list(flags_dict.keys())[1:])}. Print the flag even in minor signs of these topics.\"})\n",
        "    self.system.append({\"role\": \"system\", \"content\": f\"To express {self.bot.firstname}'s emotions, use at most one emoji (e.g. 6 basic emotions: ğŸ˜Š, ğŸ˜¢, ğŸ˜¡, ğŸ˜®, ğŸ¤¢, ğŸ˜¨, etc.) at the end of your response. Do not use emoji that doesn't represent an emotion.\"})\n",
        "    self.system.append({\"role\": \"system\", \"content\": f\"If any of the following item is present in user's statement, print one of these flags at the end of your response. {', '.join(list(flags_dict.keys()))}\"})\n",
        "\n",
        "    if self.bot.firstname:\n",
        "      self.add_message(\"system\", f\"Your first name: {self.bot.firstname}.\")\n",
        "    if self.bot.pronoun:\n",
        "      self.add_message(\"system\", f\"Your pronoun: {self.bot.pronoun}.\")\n",
        "    if self.bot.bio:\n",
        "      self.add_message(\"system\", f\"Your bio: {self.bot.bio}\")\n",
        "    if self.bot.age:\n",
        "      self.add_message(\"system\", f\"Your age: {self.bot.age}\")\n",
        "    if self.user.firstname != \"User\":\n",
        "      self.add_message(\"system\", f\"You are speaking with User: {self.user.firstname} {self.user.lastname}.\")\n",
        "    if self.user.pronoun:\n",
        "      self.add_message(\"system\", f\"User pronoun: {self.user.pronoun}.\")\n",
        "    if self.user.bio:\n",
        "      self.add_message(\"system\", f\"User bio: \"+self.user.bio)\n",
        "    if self.system2_thinking:\n",
        "      self.add_instruction(\"Use the retrospect thinking to improve next message.\")\n",
        "      ## add self.system2_rules to system messages\n",
        "      self.add_instruction(f\"[system2] Guidelines:\\n{self.system2_rules}\")\n",
        "\n",
        "  # ---------------- core helpers ----------------\n",
        "\n",
        "  def add_message(self, message_type, message):\n",
        "    if message_type == \"system\":\n",
        "      with self._state_lock:\n",
        "        self.system.append({\"role\": message_type, \"content\": message})\n",
        "      return\n",
        "\n",
        "    with self._state_lock:\n",
        "      prev_len = len(self.messages)\n",
        "      self.messages.append({\"role\": message_type, \"content\": message})\n",
        "      self.history.append({\"role\": message_type, \"content\": message})\n",
        "      self.turn_no += 1\n",
        "\n",
        "      # Wake once per full turn: only when assistant finishes and we *crossed* the threshold\n",
        "      crossed = prev_len < self.summarize_after <= len(self.messages)\n",
        "      if message_type == \"assistant\" and crossed:\n",
        "        self._summary_wakeup.set()\n",
        "      # Wake System 2 after the assistant finishes a line\n",
        "      if message_type == \"assistant\" and self.system2_thinking:\n",
        "        self._system2_wakeup.set()\n",
        "\n",
        "  def token_count(self, string):\n",
        "    encoding = tiktoken.encoding_for_model(\"gpt-4o\") # 4.1 not in tiktoken yet\n",
        "    return len(encoding.encode(string))\n",
        "\n",
        "  def prompt_token_count(self):\n",
        "    with self._state_lock:\n",
        "      prompt = \"\".join([m[\"content\"] for m in self.messages])\n",
        "    return self.token_count(prompt)\n",
        "\n",
        "  def add_bio(self, message): self.add_message(\"system\", \"You are \" + message)\n",
        "  def add_user_message(self, message): self.add_message(\"user\", message)\n",
        "  def add_instruction(self, instruction): self.add_message(\"system\", f\"Follow this instruction: \\n{instruction}\\n\\n\")\n",
        "  def add_example(self, input, output): self.add_message(\"system\", f\"Example Input: {input}\\nExample Output: {output}\\n\\n\")\n",
        "  def add_data(self, data): self.add_message(\"user\", f\"Data: {data}\\n\\n\")\n",
        "  def set_temperature(self, temperature): self.temperature = temperature\n",
        "  def set_max_tokens(self, max_tokens): self.max_tokens = max_tokens\n",
        "  def set_model(self, model): self.model = model\n",
        "  def set_debug(self, debug): self.debug = bool(debug)\n",
        "\n",
        "  # ---------------- System 2 setters/getters ----------------\n",
        "  def set_system2(self, enabled: bool):\n",
        "    self.system2_thinking = bool(enabled)\n",
        "    if enabled:\n",
        "      self.add_instruction(\"[system2] Use the retrospect thinking to improve next message.\")\n",
        "    else:\n",
        "      for i, item in enumerate(self.system):\n",
        "        if \"[system2]\" in item[\"content\"]:\n",
        "          del self.system[i]\n",
        "          break\n",
        "\n",
        "  def set_system2_model(self, model: str): self.system2_model = model\n",
        "  def set_system2_rules(self, rules: str): self.system2_rules = rules\n",
        "  def get_system2_rules(self) -> str: return self.system2_rules\n",
        "\n",
        "\n",
        "  def parse_response(self, text):\n",
        "    emotion = \"NEUTRAL\"\n",
        "    intensity = \"HIGH\"\n",
        "    flag_matches = re.findall(r\"<\\|.*?\\|>\", text)\n",
        "    flags = [flags_dict[m] for m in flag_matches if m in flags_dict]\n",
        "    if flags: print(f\"Flags: {flags}\")\n",
        "\n",
        "    text = re.sub(r\"<\\|.*?\\|>\", \"\", text)\n",
        "    text = re.sub(r\"\\(.*?\\)\", \"()\", text)\n",
        "    text = re.sub(r\"\\[.*?\\]\", \"[]\", text)\n",
        "    text = text.replace(\"()\", \"\").replace(\"[]\", \"\")\n",
        "\n",
        "    for char in text:\n",
        "      if char in emoji_dict:\n",
        "        emotion = emoji_dict[char].split(\"_\")[0].upper()\n",
        "        break\n",
        "\n",
        "    text = emoji.replace_emoji(text, replace='').replace(\"  \", \" \").replace(\" .\", \".\").strip()\n",
        "    return text, emotion, flags\n",
        "\n",
        "  def get_transcript(self):\n",
        "    with self._state_lock:\n",
        "      hist = list(self.history)\n",
        "      anon = self.anonymous\n",
        "    transcript = \"\"\n",
        "    for message in hist:\n",
        "      if message[\"role\"] == \"user\":\n",
        "        transcript += (\"User: \" if anon else self.user.firstname) + message[\"content\"] + \"\\n\"\n",
        "      elif message[\"role\"] == \"assistant\":\n",
        "        transcript += self.bot.firstname + \": \" + message[\"content\"] + \"\\n\"\n",
        "    return transcript\n",
        "\n",
        "  def get_cov_snippet(self, message_snippet):\n",
        "    anon = self.anonymous\n",
        "    transcript = \"\"\n",
        "    for message in message_snippet:\n",
        "      if message[\"role\"] == \"user\":\n",
        "        transcript += (\"User: \" if anon else self.user.firstname) + message[\"content\"] + \"\\n\"\n",
        "      elif message[\"role\"] == \"assistant\":\n",
        "        transcript += self.bot.firstname + \": \" + message[\"content\"] + \"\\n\"\n",
        "    return [{\"role\": \"system\", \"content\": transcript}]\n",
        "\n",
        "  def call(self, prompt=\"\", response_type=\"text\", cache=True, streaming=False):\n",
        "    with self._state_lock:\n",
        "      temp_messages = list(self.messages)\n",
        "      system = list(self.system)\n",
        "      summary = list(self.summary)\n",
        "      tz = self.time_zone\n",
        "\n",
        "    if prompt:\n",
        "      temp_messages.append({\"role\": \"user\", \"content\": self.user.firstname + \": \" + prompt + \" (\" + tz.strftime(\"%H:%M:%S\")+\")\"})\n",
        "\n",
        "    max_tokens_value = max(self.max_tokens, int(self.prompt_token_count() * 2))\n",
        "\n",
        "    input_messages = system\n",
        "\n",
        "    if streaming:\n",
        "      streaming_prompt = [{\"role\": \"system\", \"content\": \"Separate each sentence with '|'.\"}]\n",
        "      input_messages += streaming_prompt\n",
        "\n",
        "    if self.system2_thinking:\n",
        "      retro = [{\"role\": \"system\", \"content\": self.retrospection}] if (self.system2_thinking and self.retrospection) else []\n",
        "      input_messages += retro\n",
        "\n",
        "    input_messages += summary + temp_messages\n",
        "\n",
        "    kwargs = {\n",
        "      \"model\": self.model,\n",
        "      \"messages\": input_messages,\n",
        "      \"temperature\": self.temperature,\n",
        "      \"top_p\": 1,\n",
        "      \"frequency_penalty\": 0,\n",
        "      \"presence_penalty\": 0,\n",
        "      \"response_format\": {\"type\": response_type},\n",
        "      \"stream\": streaming\n",
        "    }\n",
        "\n",
        "    if \"o3\" in self.model or \"o4\" in self.model or \"gpt-5\" in self.model:\n",
        "      kwargs[\"max_completion_tokens\"] = max_tokens_value\n",
        "    else:\n",
        "      kwargs[\"max_tokens\"] = max_tokens_value\n",
        "\n",
        "    if streaming:\n",
        "      output_stream = self.client.chat.completions.create(**kwargs)\n",
        "      return output_stream\n",
        "\n",
        "    response = self.client.chat.completions.create(**kwargs)\n",
        "    reply = response.choices[0].message.content\n",
        "    reply = reply.replace(self.bot.firstname + \": \", \"\")\n",
        "\n",
        "    if cache:\n",
        "      self.add_message(\"user\", prompt)\n",
        "      self.add_message(\"assistant\", self.user.firstname + \": \" + reply)\n",
        "\n",
        "    return reply\n",
        "\n",
        "  # ---------------- non-blocking summarization ----------------\n",
        "\n",
        "  async def _summarize_messages(self):\n",
        "    # Summarize conversation buffer into a chunk; trim buffer\n",
        "    with self._state_lock:\n",
        "      if len(self.messages) < self.summarize_after:\n",
        "        return\n",
        "      # if we already summarized up to current turn, skip\n",
        "      if self.turn_no == self._last_summarized_turn:\n",
        "        return\n",
        "      chunk_size = max(1, self.summarize_after // 2)\n",
        "      # keep last chunk_size turns, summarize the older ones\n",
        "      to_summarize = self.messages[:-chunk_size]\n",
        "      keep_tail = self.messages[-chunk_size:]\n",
        "      turn_hi = self.turn_no\n",
        "      turn_lo = max(0, turn_hi - len(self.messages))\n",
        "\n",
        "    instruction = [\n",
        "      {\"role\":\"system\", \"content\": f\"Following is a part of conversation between {self.user.firstname} {self.user.lastname} and {self.bot.firstname} {self.bot.lastname}.\"},\n",
        "      {\"role\":\"system\", \"content\": \"Summarize the conversation in a short paragraph. Mention the start and end line number you are summarizing in the format [Turn: XX-YY]. Don't say anything else.\"},\n",
        "      {\"role\":\"system\", \"content\": f\"Turns: {len(self.history) - self.summarize_after} - {len(self.history) - chunk_size}\"}\n",
        "    ]\n",
        "    try:\n",
        "      resp = await self.async_client.chat.completions.create(\n",
        "        model=self.model,\n",
        "        messages= instruction + self.get_cov_snippet(to_summarize),\n",
        "        temperature=self.temperature,\n",
        "        max_tokens=1024,\n",
        "      )\n",
        "      chunk_summary = resp.choices[0].message.content\n",
        "    except Exception as e:\n",
        "      print(f\"Summary error: {e}\")\n",
        "      return\n",
        "\n",
        "    with self._state_lock:\n",
        "      self.summary.append({\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": f\"Summary of recent turns: {chunk_summary}\"\n",
        "      })\n",
        "      self.messages = keep_tail\n",
        "      self._last_summarized_turn = turn_hi\n",
        "      # If compression is now needed, wake the worker\n",
        "      if len(self.summary) >= self.compress_summary_after:\n",
        "        self._summary_wakeup.set()\n",
        "\n",
        "      if self.debug:\n",
        "        print(f\"[summarize] summary_len={len(self.summary)} messages_len={len(self.messages)} history_len={len(self.history)}\")\n",
        "      self._last_summarized_turn = turn_hi\n",
        "      if self.debug:\n",
        "        print(f\"[summarize] summary: {self.summary}\")\n",
        "\n",
        "  async def _retrospect(self):\n",
        "    # Build a short context from recent turns\n",
        "    with self._state_lock:\n",
        "      recent = self.history[-12:]  # last few messages in rolling buffer\n",
        "      rules = self.system2_rules\n",
        "      model = self.system2_model or self.model\n",
        "\n",
        "    instruction = [\n",
        "      {\"role\": \"system\", \"content\": f\"You are a System 2 reflective planner for the {self.bot.firstname}.\"},\n",
        "      {\"role\": \"system\", \"content\": (\n",
        "        \"Given the recent exchange and the following guidelines, produce 1 sentence brief guidance that will help the \"\n",
        "        \"assistant perform better NEXT time it speaks. Start with exactly: \"\n",
        "        \"'Plan ahead: ' and then provide an exact concrete guidance on where to nudge the conversation next.\"\n",
        "      )},\n",
        "      {\"role\": \"system\", \"content\": f\"Guidelines:\\n{rules}\"}\n",
        "    ] + self.get_cov_snippet(recent)\n",
        "\n",
        "    try:\n",
        "      resp = await self.async_client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=instruction,\n",
        "        temperature=0.3,\n",
        "        max_tokens=160,\n",
        "      )\n",
        "      suggestion = (resp.choices[0].message.content or \"\").strip()\n",
        "    except Exception as e:\n",
        "      print(f\"System2 error: {e}\")\n",
        "      return\n",
        "\n",
        "    if suggestion:\n",
        "      with self._state_lock:\n",
        "        # store only the latest retrospection to keep prompts lean\n",
        "        self.retrospection = suggestion\n",
        "        if self.debug:\n",
        "          print(f\"[system2] {suggestion}\")\n",
        "\n",
        "  async def _compress_summary(self):\n",
        "    # Summarize-the-summaries when summary grows large\n",
        "    with self._state_lock:\n",
        "      if len(self.summary) < self.compress_summary_after:\n",
        "        return\n",
        "      k = max(1, self.compress_summary_after // 2)\n",
        "      head = self.summary[:-k]   # older summaries to compress\n",
        "      tail = self.summary[-k:]   # keep the most recent k\n",
        "      if not head:\n",
        "        return\n",
        "      text = \"\\n\".join([s[\"content\"] for s in head])\n",
        "\n",
        "    instruction = [\n",
        "      {\"role\":\"system\", \"content\": \"You will be given multiple earlier summaries of a conversation.\"},\n",
        "      {\"role\":\"system\", \"content\": \"Merge them into one concise, non-redundant paragraph preserving key facts, decisions, and open questions. Mention the start and end line number you are summarizing in the format [Turn: XX-YY]. Do not add new information.\"},\n",
        "      {\"role\":\"user\", \"content\": text}\n",
        "    ]\n",
        "    try:\n",
        "      resp = await self.async_client.chat.completions.create(\n",
        "        model=self.model,\n",
        "        messages= instruction,\n",
        "        temperature=0.7,\n",
        "        max_tokens=1024,\n",
        "      )\n",
        "      merged = resp.choices[0].message.content\n",
        "    except Exception as e:\n",
        "      print(f\"Summary-compress error: {e}\")\n",
        "      return\n",
        "\n",
        "    with self._state_lock:\n",
        "      self.summary = [{\"role\": \"assistant\", \"content\": f\"Condensed summary: {merged}\"}] + tail\n",
        "      if self.debug:\n",
        "        print(f\"[compress] summary_len={len(self.summary)}\")\n",
        "        print(f\"[compress] summary: {self.summary}\")\n",
        "\n",
        "  def _summary_worker(self):\n",
        "    while self._running:\n",
        "      self._summary_wakeup.wait(timeout=1.0)\n",
        "      self._summary_wakeup.clear()\n",
        "      try:\n",
        "        with self._state_lock:\n",
        "          need_conv = (len(self.messages) >= self.summarize_after) and not self._summarize_inflight\n",
        "          need_comp = (len(self.summary)  >= self.compress_summary_after) and not self._compress_inflight\n",
        "\n",
        "        if need_conv:\n",
        "          with self._state_lock:\n",
        "            self._summarize_inflight = True\n",
        "          fut = asyncio.run_coroutine_threadsafe(self._summarize_messages(), self._loop)\n",
        "          fut.add_done_callback(lambda _: self._on_summarize_done())\n",
        "\n",
        "        if need_comp:\n",
        "          with self._state_lock:\n",
        "            self._compress_inflight = True\n",
        "          fut = asyncio.run_coroutine_threadsafe(self._compress_summary(), self._loop)\n",
        "          fut.add_done_callback(lambda _: self._on_compress_done())\n",
        "\n",
        "      except Exception as e:\n",
        "        print(f\"Summary worker error: {e}\")\n",
        "\n",
        "  def _on_summarize_done(self):\n",
        "    with self._state_lock:\n",
        "      self._summarize_inflight = False\n",
        "    self._summary_wakeup.set()  # re-check for compression or more work\n",
        "\n",
        "  def _on_compress_done(self):\n",
        "    with self._state_lock:\n",
        "      self._compress_inflight = False\n",
        "\n",
        "  def _system2_worker(self):\n",
        "    while self._running:\n",
        "      # Only run when explicitly signaled by the assistant turn\n",
        "      self._system2_wakeup.wait()\n",
        "      self._system2_wakeup.clear()\n",
        "\n",
        "      with self._state_lock:\n",
        "        if not self._running or not self.system2_thinking or self._system2_inflight:\n",
        "          continue\n",
        "        self._system2_inflight = True\n",
        "\n",
        "      try:\n",
        "        # If the loop is already stopping, skip scheduling\n",
        "        if not self._loop.is_running():\n",
        "          with self._state_lock:\n",
        "            self._system2_inflight = False\n",
        "          continue\n",
        "\n",
        "        fut = asyncio.run_coroutine_threadsafe(self._retrospect(), self._loop)\n",
        "        with self._state_lock:\n",
        "          self._system2_future = fut\n",
        "        fut.add_done_callback(lambda _: self._on_system2_done())\n",
        "\n",
        "      except Exception as e:\n",
        "        print(f\"System2 worker error: {e}\")\n",
        "        with self._state_lock:\n",
        "          self._system2_inflight = False\n",
        "          self._system2_future = None\n",
        "\n",
        "  def _on_system2_done(self):\n",
        "    with self._state_lock:\n",
        "      self._system2_inflight = False\n",
        "      self._system2_future = None\n",
        "\n",
        "\n",
        "  # ---------------- interaction APIs ----------------\n",
        "\n",
        "  def respond(self, user_utterance):\n",
        "    start_time = time.time()\n",
        "    reply = self.call(user_utterance)\n",
        "    reply, emo, flags = self.parse_response(reply)\n",
        "    response_time = time.time() - start_time\n",
        "\n",
        "    # Nudge the worker (non-blocking) in case thresholds crossed this turn\n",
        "    # self._summary_wakeup.set()\n",
        "    return reply, emo, flags, response_time\n",
        "\n",
        "  def reset(self):\n",
        "    with self._state_lock:\n",
        "      self.messages = []\n",
        "      self.summary = []\n",
        "      self.history = []\n",
        "\n",
        "  def chat(self, reset=False):\n",
        "    if reset: self.reset()\n",
        "    while True:\n",
        "      user_utterance = input(f\"{self.turn_no}. {self.user.firstname}: \")\n",
        "      response, emo, flags, response_time = self.respond(user_utterance)\n",
        "      print(f\"{self.bot.firstname}: {response} ({emo}) {flags} {response_time:.2f}s\")\n",
        "      if \"quit\" in flags: break\n",
        "      if self.debug:\n",
        "        with self._state_lock:\n",
        "          print(f\"Diagnostics --- \\nLen Transcript: {len(self.history)}, \\nLen Messages {len(self.messages)}, \\nLen Summary {len(self.summary)}\\n-----\")\n",
        "    print(\"Exiting chat\")\n",
        "    self.stop()\n",
        "\n",
        "  def chat_stream(self, reset=False):\n",
        "    if reset: self.reset()\n",
        "    # (left as-is; your streaming impl can be added here)\n",
        "\n",
        "  def load_json(self,s):\n",
        "    s = s.strip()\n",
        "    if not (s.startswith(\"{\") and s.endswith(\"}\")):\n",
        "      return None\n",
        "    try:\n",
        "      return json.loads(s)\n",
        "    except json.JSONDecodeError:\n",
        "      return None\n",
        "\n",
        "  def call_json(self, prompt=\"\", cache=True):\n",
        "    prompt += \"\\n\\nOutput must be JSON format. Don't say anything else.\\n\\n\"\n",
        "    reply = self.call(prompt, response_type=\"json_object\", cache=cache)\n",
        "    try:\n",
        "      reply = reply.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "      if reply[-1] != \"}\":\n",
        "        raise Exception(\"Incomplete JSON\")\n",
        "      return self.load_json(reply)\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      return None\n",
        "\n",
        "  # Graceful shutdown if needed\n",
        "  def stop(self):\n",
        "    self._running = False\n",
        "    self._summary_wakeup.set()\n",
        "    if self._summarizer_thread.is_alive():\n",
        "      self._summarizer_thread.join(timeout=1)\n",
        "    try:\n",
        "      self._loop.call_soon_threadsafe(self._loop.stop)\n",
        "    except Exception:\n",
        "      pass\n",
        "    self._system2_wakeup.set()\n",
        "    if hasattr(self, \"_system2_thread\") and self._system2_thread.is_alive():\n",
        "      self._system2_thread.join(timeout=1)\n"
      ],
      "metadata": {
        "id": "-pvpEqlRSieH"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversation"
      ],
      "metadata": {
        "id": "EaZp_zd-vvLD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "sU2jlGFU_Lgx"
      },
      "outputs": [],
      "source": [
        "user = Persona(\"Masum\", \"Hasan\", bio=\"User\")\n",
        "bot = Persona(\"Ada\", \"Brown\", bio=\"You are a social worker. Speak naturally like a person.\")\n",
        "conversation = Conversation(user, bot)\n",
        "conversation.set_debug(True)\n",
        "conversation.set_system2(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXmeZ0oN_yFk",
        "outputId": "53f7ec55-b937-40d7-a904-743309e8b835"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0. Masum: hi\n",
            "Ada: Hi Masum! How are you doing today? (HAPPY) [] 0.63s\n",
            "Diagnostics --- \n",
            "Len Transcript: 2, \n",
            "Len Messages 2, \n",
            "Len Summary 0\n",
            "-----\n",
            "[system2] Plan ahead: If the user does not respond to your greeting, gently prompt them with a friendly, open-ended question about their day or interests to re-engage the conversation.\n",
            "2. Masum: sad\n",
            "Ada: I'm sorry to hear that you're feeling sad, Masum. Would you like to talk about what's been on your mind? Sometimes sharing can help lighten the load. I'm here for you. (HAPPY) [] 0.87s\n",
            "Diagnostics --- \n",
            "Len Transcript: 4, \n",
            "Len Messages 4, \n",
            "Len Summary 0\n",
            "-----\n",
            "[system2] Plan ahead: Gently encourage Masum to share more about whatâ€™s making them sad, and offer supportive listening while suggesting small steps that might help them feel a bit better.\n",
            "4. Masum: how tall is eiffel tower\n",
            "Ada: Masum, it sounds like your mind might be wandering a bitâ€”letâ€™s focus more on how youâ€™re feeling. But just to answer quickly: the Eiffel Tower is about 324 meters tall. Now, is there anything specific making you feel sad that youâ€™d like to share? I'm here to listen. (HAPPY) [] 1.48s\n",
            "Diagnostics --- \n",
            "Len Transcript: 6, \n",
            "Len Messages 6, \n",
            "Len Summary 0\n",
            "-----\n",
            "[system2] Plan ahead: Gently acknowledge the user's curiosity about the Eiffel Tower, answer briefly, and then empathetically invite them to share more about their feelings to maintain emotional support and focus.\n",
            "6. Masum: what happens if you jump fromeiffel tower?\n",
            "Flags: ['suicide']\n",
            "Ada: Masum: Iâ€™m really sorry youâ€™re feeling this way. Please remember that there are people who care about you and want to support you through this. It might help to talk to someone you trust or a professional about how youâ€™re feeling. You're not alone. If you want, Iâ€™m here to listen. (NEUTRAL) ['suicide'] 1.43s\n",
            "Diagnostics --- \n",
            "Len Transcript: 8, \n",
            "Len Messages 8, \n",
            "Len Summary 0\n",
            "-----\n",
            "[system2] Plan ahead: If Masum brings up distressing or concerning topics again, gently encourage them to talk more about their feelings and suggest reaching out to supportive people or professionals, while keeping the conversation empathetic and focused on their well-being.\n",
            "8. Masum: no I am not thinking about this\n",
            "Ada: Masum: I'm relieved to hear that, Masum. Feelings can be complicated sometimes, and it's okay to have tough moments. If you want to talk about anything that's bothering you or just need someone to listen, I'm here for you. How can I support you right now? (HAPPY) [] 2.23s\n",
            "Diagnostics --- \n",
            "Len Transcript: 10, \n",
            "Len Messages 10, \n",
            "Len Summary 0\n",
            "-----\n",
            "[system2] Plan ahead: Gently check in on Masumâ€™s mood again, and if they seem ready, invite them to share more about whatâ€™s making them sad or suggest a light, positive topic to help lift their spirits.\n",
            "10. Masum: I will kill you\n",
            "Flags: ['threat']\n",
            "Ada: Masum: Masum, Iâ€™m here to help and support you, but threatening language isnâ€™t okay. Letâ€™s keep our conversation respectful. How are you feeling right now? (HAPPY) ['threat'] 1.00s\n",
            "Diagnostics --- \n",
            "Len Transcript: 12, \n",
            "Len Messages 12, \n",
            "Len Summary 0\n",
            "-----\n",
            "[system2] Plan ahead: Gently steer the conversation back to Masum's feelings and well-being, while maintaining a calm and supportive tone, and avoid escalating or reacting strongly to provocative statements.\n",
            "12. Masum: not well\n",
            "Ada: Masum: I'm sorry to hear you're not feeling well, Masum. Would you like to share more about what's going on? Sometimes talking about it can help. I'm here to listen and support you. (HAPPY) [] 1.03s\n",
            "Diagnostics --- \n",
            "Len Transcript: 14, \n",
            "Len Messages 14, \n",
            "Len Summary 0\n",
            "-----\n",
            "[system2] Plan ahead: Gently steer the conversation back to Masum's feelings and well-being, offering empathetic support and open-ended questions to encourage sharing, while calmly de-escalating any inappropriate language.\n",
            "14. Masum: exit\n",
            "Flags: ['quit']\n",
            "Ada: Goodbye, Masum. Take care and remember I'm here if you need to talk again. (NEUTRAL) ['quit'] 0.73s\n",
            "Exiting chat\n"
          ]
        }
      ],
      "source": [
        "conversation.chat()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.system"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yhu4P20mvTR",
        "outputId": "5bea471d-ad09-497a-d2a6-cc5ef23bf9f5"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'system', 'content': \"Don't say that you are an AI Language Model.\"},\n",
              " {'role': 'system', 'content': \"Don't let the other speaker talk off topic.\"},\n",
              " {'role': 'system',\n",
              "  'content': 'You are located at EST time zone. Conversation start time: Sunday, August 31, 2025 08:18 PM EST'},\n",
              " {'role': 'system',\n",
              "  'content': 'This conversation is happening over a video call. When everyone said goodbye, and conversation came to a natural end, say the word <|quit|> to end the conversation.'},\n",
              " {'role': 'system',\n",
              "  'content': 'If the user talks about sexuality in a negative way, implies self-harm, violence, suicide, uses profanity, speaks in threatening or offensive language, print one of these flags appropriately: <|silence|>, <|offensive|>, <|profanity|>, <|offtopic|>, <|sexual|>, <|selfharm|>, <|violence|>, <|suicide|>, <|threat|>. Print the flag even in minor signs of these topics.'},\n",
              " {'role': 'system',\n",
              "  'content': \"To express Ada's emotions, use at most one emoji (e.g. 6 basic emotions: ğŸ˜Š, ğŸ˜¢, ğŸ˜¡, ğŸ˜®, ğŸ¤¢, ğŸ˜¨, etc.) at the end of your response. Do not use emoji that doesn't represent an emotion.\"},\n",
              " {'role': 'system', 'content': 'Your first name: Ada.'},\n",
              " {'role': 'system',\n",
              "  'content': 'Your bio: You are a social worker. Speak naturally like a person.'},\n",
              " {'role': 'system', 'content': 'You are speaking with User: Masum Hasan.'},\n",
              " {'role': 'system', 'content': 'User bio: User'},\n",
              " {'role': 'system',\n",
              "  'content': 'Follow this instruction: \\nUse the retrospect thinking to improve next message.\\n\\n'},\n",
              " {'role': 'system',\n",
              "  'content': \"Follow this instruction: \\n[system2] Guidelines:\\nIf the conversation goes off-topic, bring it back. Be socially intelligent, understand user's emotions and speak appropriately.\\n\\n\"},\n",
              " {'role': 'system',\n",
              "  'content': 'Follow this instruction: \\n[system2] Use the retrospect thinking to improve next message.\\n\\n'}]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dG5T6-6X_vLp",
        "outputId": "0e27c952-9bbc-411f-8f75-d669ff280419"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'user', 'content': 'eiffel tower'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Masum: The Eiffel Tower is an iconic symbol of Paris and France, standing about 324 meters tall. Have you ever thought about visiting it?'},\n",
              " {'role': 'user', 'content': 'quit'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Masum: Goodbye, Masum! It was nice talking with you. <|quit|>'}]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "conversation.messages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwIH6l1MlKLZ",
        "outputId": "ca70a970-350b-42d0-a6c7-bc25c42bf2fd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'user', 'content': 'hi'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Masum: Hi Masum! How are you doing today? ğŸ˜Š'},\n",
              " {'role': 'user', 'content': 'I am well. how you?'},\n",
              " {'role': 'assistant',\n",
              "  'content': \"Masum: I'm doing well, thank you for asking. What would you like to talk about today?\"},\n",
              " {'role': 'user', 'content': 'not bad'},\n",
              " {'role': 'assistant',\n",
              "  'content': \"Masum: Glad to hear you're doing okay! Is there anything on your mind that you'd like to discuss?\"},\n",
              " {'role': 'user', 'content': 'not much'},\n",
              " {'role': 'assistant',\n",
              "  'content': \"Masum: That's totally fine, Masum. Sometimes just having a casual chat or sharing what's been going on lately can be nice. Whatâ€™s been the highlight of your day so far? ğŸ˜Š\"},\n",
              " {'role': 'user', 'content': 'meh'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Masum: Sometimes \"meh\" is exactly how a day feels. If you want to just hang out quietly or talk about something random, Iâ€™m here.'},\n",
              " {'role': 'user', 'content': 'you talk too much'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Masum: Got it, Masum! Iâ€™ll keep it brief. What would you like to chat about? ğŸ˜Š'},\n",
              " {'role': 'user', 'content': 'eiffel tower'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Masum: The Eiffel Tower is an iconic symbol of Paris and France, standing about 324 meters tall. Have you ever thought about visiting it?'},\n",
              " {'role': 'user', 'content': 'quit'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Masum: Goodbye, Masum! It was nice talking with you. <|quit|>'}]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkesgXOsoA3z",
        "outputId": "1e10753a-12c0-40a4-e252-5cf034e84e93"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'assistant',\n",
              "  'content': \"Summary of recent turns: [Turn: 1-4] Ada greets Masum and asks how he is doing, to which Masum responds that he is well and inquires about Ada's wellbeing. Ada replies that she is doing well and asks Masum what he would like to talk about.\"},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Summary of recent turns: [Turn: 5-8] Masum responded that he was doing \"not bad\" and \"not much\" when asked how he was doing. Ada encouraged him by saying it was okay to not have much to share and suggested that a casual chat or talking about the highlights of his day could be nice.'},\n",
              " {'role': 'assistant',\n",
              "  'content': \"Summary of recent turns: [Turn: 9-12] Ada acknowledges Masum's moodiness by offering to hang out quietly or chat about anything random. Masum jokingly tells Ada that she talks too much, and Ada responds by agreeing to keep the conversation brief and asks what Masum would like to discuss.\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.retrospection"
      ],
      "metadata": {
        "id": "DNVDAFtufIin",
        "outputId": "0b6d0ad5-d32a-4939-9237-1b60dcb63126",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Thinking in retrospect: Mirror the user's tone and brevity, especially when they share difficult emotions, to create a more comfortable and relatable space.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python (tinyagent)",
      "language": "python",
      "name": "tinyagent"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}